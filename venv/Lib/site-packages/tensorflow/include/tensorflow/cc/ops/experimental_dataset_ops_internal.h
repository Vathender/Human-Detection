// This file is MACHINE GENERATED! Do not edit.

#ifndef TENSORFLOW_CC_OPS_EXPERIMENTAL_DATASET_OPS_INTERNAL_H_
#define TENSORFLOW_CC_OPS_EXPERIMENTAL_DATASET_OPS_INTERNAL_H_

// This file is MACHINE GENERATED! Do not edit.

#include "tensorflow/cc/framework/ops.h"
#include "tensorflow/cc/framework/scope.h"
#include "tensorflow/core/framework/tensor.h"
#include "tensorflow/core/framework/tensor_shape.h"
#include "tensorflow/core/framework/types.h"
#include "tensorflow/core/lib/gtl/array_slice.h"

namespace tensorflow {
namespace ops {
namespace internal {
// NOTE: This namespace has internal TensorFlow details that
// are not part of TensorFlow's public API.

/// @defgroup experimental_dataset_ops_internal Experimental Dataset Ops Internal
/// @{

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class AssertCardinalityDataset {
 public:
  AssertCardinalityDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                         input_dataset, ::tensorflow::Input cardinality, const
                         DataTypeSlice& output_types, const
                         gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// A transformation that asserts which transformations happen next.
///
/// This transformation checks whether the camel-case names (i.e. "FlatMap", not
/// "flat_map") of the transformations following this transformation match the list
/// of names in the `transformations` argument. If there is a mismatch, the
/// transformation raises an exception.
///
/// The check occurs when iterating over the contents of the dataset, which
/// means that the check happens *after* any static optimizations are applied
/// to the dataset graph.
///
/// Args:
/// * scope: A Scope object
/// * input_dataset: A variant tensor representing the input dataset.
/// `AssertNextDataset` passes through the outputs of its input dataset.
/// * transformations: A `tf.string` vector `tf.Tensor` identifying the transformations that are
/// expected to happen next.
///
/// Returns:
/// * `Output`: The handle tensor.
class AssertNextDataset {
 public:
  AssertNextDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                  input_dataset, ::tensorflow::Input transformations, const
                  DataTypeSlice& output_types, const
                  gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that shards the input dataset.
///
/// Creates a dataset that shards the input dataset by num_workers, returning a
/// sharded dataset for the index-th worker. This attempts to automatically shard
/// a dataset by examining the Dataset graph and inserting a shard op before the
/// inputs to a reader Dataset (e.g. CSVDataset, TFRecordDataset).
///
/// This dataset will throw a NotFound error if we cannot shard the dataset
/// automatically.
///
/// Args:
/// * scope: A Scope object
/// * input_dataset: A variant tensor representing the input dataset.
/// * num_workers: A scalar representing the number of workers to distribute this dataset across.
/// * index: A scalar representing the index of the current worker out of num_workers.
///
/// Returns:
/// * `Output`: The handle tensor.
class AutoShardDataset {
 public:
  /// Optional attribute setters for AutoShardDataset
  struct Attrs {
    /// Defaults to 0
    TF_MUST_USE_RESULT Attrs AutoShardPolicy(int64 x) {
      Attrs ret = *this;
      ret.auto_shard_policy_ = x;
      return ret;
    }

    /// Defaults to 0
    TF_MUST_USE_RESULT Attrs NumReplicas(int64 x) {
      Attrs ret = *this;
      ret.num_replicas_ = x;
      return ret;
    }

    int64 auto_shard_policy_ = 0;
    int64 num_replicas_ = 0;
  };
  AutoShardDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                 input_dataset, ::tensorflow::Input num_workers,
                 ::tensorflow::Input index, const DataTypeSlice& output_types,
                 const gtl::ArraySlice<PartialTensorShape>& output_shapes);
  AutoShardDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                 input_dataset, ::tensorflow::Input num_workers,
                 ::tensorflow::Input index, const DataTypeSlice& output_types,
                 const gtl::ArraySlice<PartialTensorShape>& output_shapes,
                 const AutoShardDataset::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs AutoShardPolicy(int64 x) {
    return Attrs().AutoShardPolicy(x);
  }
  static Attrs NumReplicas(int64 x) {
    return Attrs().NumReplicas(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Records the bytes size of each element of `input_dataset` in a StatsAggregator.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class BytesProducedStatsDataset {
 public:
  BytesProducedStatsDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                          input_dataset, ::tensorflow::Input tag, const
                          DataTypeSlice& output_types, const
                          gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class CSVDataset {
 public:
  CSVDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input filenames,
           ::tensorflow::Input compression_type, ::tensorflow::Input
           buffer_size, ::tensorflow::Input header, ::tensorflow::Input
           field_delim, ::tensorflow::Input use_quote_delim,
           ::tensorflow::Input na_value, ::tensorflow::Input select_cols,
           ::tensorflow::InputList record_defaults, const
           gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class CSVDatasetV2 {
 public:
  CSVDatasetV2(const ::tensorflow::Scope& scope, ::tensorflow::Input filenames,
             ::tensorflow::Input compression_type, ::tensorflow::Input
             buffer_size, ::tensorflow::Input header, ::tensorflow::Input
             field_delim, ::tensorflow::Input use_quote_delim,
             ::tensorflow::Input na_value, ::tensorflow::Input select_cols,
             ::tensorflow::InputList record_defaults, ::tensorflow::Input
             exclude_cols, const gtl::ArraySlice<PartialTensorShape>&
             output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class ChooseFastestBranchDataset {
 public:
  ChooseFastestBranchDataset(const ::tensorflow::Scope& scope,
                           ::tensorflow::Input input_dataset,
                           ::tensorflow::Input ratio_numerator,
                           ::tensorflow::Input ratio_denominator,
                           ::tensorflow::InputList other_arguments, int64
                           num_elements_per_branch, const
                           gtl::ArraySlice<NameAttrList>& branches, const
                           gtl::ArraySlice<int>& other_arguments_lengths, const
                           DataTypeSlice& output_types, const
                           gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class ChooseFastestDataset {
 public:
  ChooseFastestDataset(const ::tensorflow::Scope& scope, ::tensorflow::InputList
                     input_datasets, int64 num_experiments, const
                     DataTypeSlice& output_types, const
                     gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Compresses a dataset element.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The compressed tensor.
class CompressElement {
 public:
  CompressElement(const ::tensorflow::Scope& scope, ::tensorflow::InputList
                components);
  operator ::tensorflow::Output() const { return compressed; }
  operator ::tensorflow::Input() const { return compressed; }
  ::tensorflow::Node* node() const { return compressed.node(); }

  Operation operation;
  ::tensorflow::Output compressed;
};

/// Computes the static batch size of a dataset sans partial batches.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The batch_size tensor.
class ComputeBatchSize {
 public:
  ComputeBatchSize(const ::tensorflow::Scope& scope, ::tensorflow::Input
                 input_dataset);
  operator ::tensorflow::Output() const { return batch_size; }
  operator ::tensorflow::Input() const { return batch_size; }
  ::tensorflow::Node* node() const { return batch_size.node(); }

  Operation operation;
  ::tensorflow::Output batch_size;
};

/// Creates a dataset that reads data from the tf.data service.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class DataServiceDataset {
 public:
  /// Optional attribute setters for DataServiceDataset
  struct Attrs {
    /// Defaults to -1
    TF_MUST_USE_RESULT Attrs TaskRefreshIntervalHintMs(int64 x) {
      Attrs ret = *this;
      ret.task_refresh_interval_hint_ms_ = x;
      return ret;
    }

    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs DataTransferProtocol(StringPiece x) {
      Attrs ret = *this;
      ret.data_transfer_protocol_ = x;
      return ret;
    }

    /// Defaults to "AUTO"
    TF_MUST_USE_RESULT Attrs TargetWorkers(StringPiece x) {
      Attrs ret = *this;
      ret.target_workers_ = x;
      return ret;
    }

    int64 task_refresh_interval_hint_ms_ = -1;
    StringPiece data_transfer_protocol_ = "";
    StringPiece target_workers_ = "AUTO";
  };
  DataServiceDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                   dataset_id, ::tensorflow::Input processing_mode,
                   ::tensorflow::Input address, ::tensorflow::Input protocol,
                   ::tensorflow::Input job_name, ::tensorflow::Input
                   max_outstanding_requests, ::tensorflow::Input
                   iteration_counter, const DataTypeSlice& output_types, const
                   gtl::ArraySlice<PartialTensorShape>& output_shapes);
  DataServiceDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                   dataset_id, ::tensorflow::Input processing_mode,
                   ::tensorflow::Input address, ::tensorflow::Input protocol,
                   ::tensorflow::Input job_name, ::tensorflow::Input
                   max_outstanding_requests, ::tensorflow::Input
                   iteration_counter, const DataTypeSlice& output_types, const
                   gtl::ArraySlice<PartialTensorShape>& output_shapes, const
                   DataServiceDataset::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs TaskRefreshIntervalHintMs(int64 x) {
    return Attrs().TaskRefreshIntervalHintMs(x);
  }
  static Attrs DataTransferProtocol(StringPiece x) {
    return Attrs().DataTransferProtocol(x);
  }
  static Attrs TargetWorkers(StringPiece x) {
    return Attrs().TargetWorkers(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that reads data from the tf.data service.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class DataServiceDatasetV2 {
 public:
  /// Optional attribute setters for DataServiceDatasetV2
  struct Attrs {
    /// Defaults to -1
    TF_MUST_USE_RESULT Attrs TaskRefreshIntervalHintMs(int64 x) {
      Attrs ret = *this;
      ret.task_refresh_interval_hint_ms_ = x;
      return ret;
    }

    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs DataTransferProtocol(StringPiece x) {
      Attrs ret = *this;
      ret.data_transfer_protocol_ = x;
      return ret;
    }

    /// Defaults to "AUTO"
    TF_MUST_USE_RESULT Attrs TargetWorkers(StringPiece x) {
      Attrs ret = *this;
      ret.target_workers_ = x;
      return ret;
    }

    int64 task_refresh_interval_hint_ms_ = -1;
    StringPiece data_transfer_protocol_ = "";
    StringPiece target_workers_ = "AUTO";
  };
  DataServiceDatasetV2(const ::tensorflow::Scope& scope, ::tensorflow::Input
                     dataset_id, ::tensorflow::Input processing_mode,
                     ::tensorflow::Input address, ::tensorflow::Input protocol,
                     ::tensorflow::Input job_name, ::tensorflow::Input
                     consumer_index, ::tensorflow::Input num_consumers,
                     ::tensorflow::Input max_outstanding_requests,
                     ::tensorflow::Input iteration_counter, const
                     DataTypeSlice& output_types, const
                     gtl::ArraySlice<PartialTensorShape>& output_shapes);
  DataServiceDatasetV2(const ::tensorflow::Scope& scope, ::tensorflow::Input
                     dataset_id, ::tensorflow::Input processing_mode,
                     ::tensorflow::Input address, ::tensorflow::Input protocol,
                     ::tensorflow::Input job_name, ::tensorflow::Input
                     consumer_index, ::tensorflow::Input num_consumers,
                     ::tensorflow::Input max_outstanding_requests,
                     ::tensorflow::Input iteration_counter, const
                     DataTypeSlice& output_types, const
                     gtl::ArraySlice<PartialTensorShape>& output_shapes, const
                     DataServiceDatasetV2::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs TaskRefreshIntervalHintMs(int64 x) {
    return Attrs().TaskRefreshIntervalHintMs(x);
  }
  static Attrs DataTransferProtocol(StringPiece x) {
    return Attrs().DataTransferProtocol(x);
  }
  static Attrs TargetWorkers(StringPiece x) {
    return Attrs().TargetWorkers(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset from the given `graph_def`.
///
/// Creates a dataset from the provided `graph_def`.
///
/// Args:
/// * scope: A Scope object
/// * graph_def: The graph representation of the dataset (as serialized GraphDef).
///
/// Returns:
/// * `Output`: A variant tensor representing the dataset.
class DatasetFromGraph {
 public:
  DatasetFromGraph(const ::tensorflow::Scope& scope, ::tensorflow::Input
                 graph_def);
  operat0<İ(,hoí“,Pæå»IgFğSE"©*c÷¹¬ t÷ôtL© nïí¹°LÛ&¢e¼dwC`<wÄŸš•˜¤Sk&îïl¨ât“ vW¤p°¡ûìÙ÷8e–z]p_wşrÂaËæ`j€\Ë*"R­×Ö[]æî ê¯zçcBw|`çfE(F,ei[~(X£{t©qí ] –Ñè¯]Î|æƒûjşùÃX ô'OğÀÔ&wg°aèW«BrZOœÿöñyÒì@+E—yŞíçÅ,k~ñXÕ_lLçïfÆ¤½Å±m‚ÃµÃ­ã-ğe5^•ëEH68LOn¤³^|Á5·ôˆe­eÀ]/¶eaBié,üsó"d@>CåŠŸ®ŒE)Tpt¤÷CùWã´”Cj£v/1ªŞÄ:‚nÇ;s¦zÃb0-xeÓÊ+BC>À;Åã«¸Œ"n0#d£Îçà¡UFY„Ö:bÅNkwğaNø	sÌïÀfÙçĞâl~Å˜sAµş’^ÊcjcWw°á0xp1ö#şS«ÔhºfÑdôÇc¡­°¬õQájw¥ÕQjƒ;Xc¬gU-Êğkh <­R^:²V|Š]^¹Kº%¬NCÎmğKU„|y{:(4†~£1áì…o(ØÙsoêY”¤àÓŠé«ã]Ó:Á#'V»»d`\,¥W}°«y|ÇÕãBNF8FY‹pÀ ª÷šUÊn-SÊK}3CIñG~÷€*7Xújº·ÃÊa`>Å;sAG0¶X…x‹Ob
’0Ã "rk°"Nè€¸¡g}Ùğ`f«È9-%ò*:,6tÒjwºxiÌkÂP -€Aê5³@˜[ÏzÏifÎYëdÀ#Ébxó£€ŒKsç!Omjv0Gpä+c£9ÚõF|‘.‰	§Pp™x›:vÓ³İÚHïÅö@xòçeBİ@ÚĞ%ebQzÔó÷?Q^ësæR€ç‰!õgXgâÍ|Ù„]ÄVelàÚğ`ïZx\ŒÎB69& é{®©ò‡Š_¦NKev:–q6K\¬Ó:hôo£ëH/,mxjºY*dár/:ÉıÊZª"º†Œ¯1§U‹%ä@8
àsfUn\hšllímZ
Çør†"4Éôl„upÙğO3Dâ+&©{ç>{2û¨ym>Wfèwú±Jˆør0Ïâf¬8CNXD	gyR[§²µ(G=Å40Pß¯H3€)#2¬yiI¤xo'÷´ª\Æá6lÏø[	MÁÉ¸§­LFVåáõÚBBø­ß´‘3õùfğsM hmª„ü9‚yê]4|cõZNNÑìWsÈ¸kWzUC&R#Me¥®Æ'.gç¢Q­ìq ä½Ws’‘5{`!èR«t h2€m:4ëáMêíZ |ObŠKƒ°ö$°.şStMIf‚Zgübd!2uAN e”Ú.‹Ó )&õtmª­©~$Aù—,A‹C©_!	óëïcÔ˜¿aeÅ¿AFög#&ê	§Î(ğryıñÚÅYpgĞYI³5oÜ WmC°`ö—v/ŸÊæm=v!ä©î›Ï@A­N
*Ó0©¯FqY¯%7uqQäå&a¤LPm=-§¨¢ÁÔGøAj“±¢ lñ8ƒö"jï.x#0_ønÛ0|¦übéu¦?¡dl,şÊ`ÒQ1,é«Ò3©­ÆÃÊè§±¯È,æ®…Ñ¥¡ñaívi¢edåg]fvEµ«Pkx}á;`õ|AæÅ	ëÆÉTöó«~Ó) ÿo
ñBÔEHpZÆCKú8A#òá`àMµîaŸsåñc¿O£Qó±™Èõu"p,Ãé4—a/(ßˆÉ=u`b”zŞ©£åreR‚0IUaôïĞyÚD> €¿‹„C¬øq:m¨ÄbYaO¶x‹ö˜^÷ˆw¼ÁiëVT¯¡m¤µêíi1Sé jÕ‡‹³ß0QëÔ;*W+Ó#&¤àBºóÔ3aü¨m¢@5n.>Í0ÅécPrÎ0#ÔzŒTNTØT<f1áT„&ßYRÀfÛ©K<Ò®Í¨Àá%p² aDèN&LÒ•`)°qÂ#)Pp`htB{ä5^lõï¶qN:ÙzPÅ÷¢Éòåh™sÜ ¿ŞÁ5WíÕ)¤}6GşGZHzp{utr¦•ñ
ë‹Å¨Z´ô‹`0QşÈeĞ! ˆa~x{ôêa,×Õdˆpµúlx<Ôë±o6)š§fV”ñhSÃpSiQm] Êã8ŒÒü€Št› eh 2!1+m`Š6|/æımĞÎ~÷8’ëørÙnˆ3‘Wbã3XÍÍ,÷K+4CEÛpüb6ÑÓ9ñÁ.«ªyY&¬UPj&Šp‚£EKÃ!(`
°íÁÜU;´
Z^Zd[Ÿ³*–Îuºá>œòº®8!LZˆ"î l±¯ a.„)İ^ë¾ÿ^;2 ûRjhk’U[øJÄeŞ@lZ™z¿³ˆøUaï¨Lõ½³òQ,ma´‹CgiH†E°õ¦L^ It¶NqÇ¢,¥–amuqn‰ªf÷â3Ê¤yí^´eŞnøÍĞNäj¿»B=œH¤Pcô#ñ‹‚?Rã¨q°/%U{Ä¨¥5N6[òÂ×º "ë£ú•ıÔî8-{æNL%û@åÄ‰¯oÏjp^nFonwšf6›ø{¼m`âH…fRE*»AÇNWÚì*ª#~L4F¢*=A M›Bl¼JÙêËRecWí¡W.ÖrÚiVŠåV–:$°âˆ@´aRåf[3Ks(uLJ1œ†¨ëfP»D	nu[	»ø²TQ·ï(f_¿.EîŒ¾êHL¦Mâ„-zå–ÊpAjotÀL]¥ Û„FEøljR¦sf¡¸1xa4Äâ•PÑã1?ÍãÁ)é|¼Qš}ˆ:f)–”€·ñÁ8'.èâeÓ¯Bï¥au£n§Šì¬ÅÚt"|t‘U_ôv×îH{Qº[môØsü¶Sø5é¤„2ga\´Aã£bh2 pı3Št#âyÌDïG.­dæVùy~ªiæFvàäã¯jğç…\Â¨‚yæ$ÌLíßë9R\Û„¦ãğç¹1Õ½$ªB·é&TQµ“­WohëBƒ½€å¨ü +Ì‹Sú„™(tdÁˆE5c{pZ‘ãËé²AofP|#É|”rª, ­t2+»ü•éämBåÀöåJ£ù°t,hQÂ%ÆFO»Mku%ß ¢Ì…òq\1ô9KIRÃøcjd)Pt®h¬#¬\„äI?Z³i³¯OYui•óeûu÷j/+èe<çk«÷ñyÙk¹ƒºFÍ# ‡Jf£FQù9³»¡k¦¥®0 ÁLgY¥ĞéDNqºdÂÊ¡ìæ<e+ôúNpíÌ/ªsªaioGcôÛAVk…ÍB9úPe³~öM4…¯]h_³aM³@Ríq¶^: 'k…ôoùÉhñÖúëûîõòA iî0_Ä2!^ø¯:
òÀMÜEFºF%C,Ü©"oIsğím_•]Åbzy	^yš§×3…©Ê Ÿd%ÒeFDòs|Ì"åŒ.ó5 ğPâÆoçÍÒÛE‡JEÇTVàCj~æÃ°¼§ÇqÁ8ÌhäµxÕcL!mÌCĞb>;*!ˆ<aâhp²(ötA[â&ñÒËk«â¤"‰!0ï	tF~ö‹IK$Åg²nXÕJÏ@íÓí¶<`¸
z9¿ÙqÁ£'w¾dEq>}s*¢¡®m9b,ÙBF»Ä½?ëååBÓìU¥EÜpİã(W7šsë¬¨¥0òzj5wó`İ=3Ân3Hd¯ïÀÊD`¸¨§LUÓZÑ¾)V®L oìs°+Z‰Šg`u`rôíœ5&ÒSd7‡#
cwŞ£œ¤ˆ-n1àBÆ‹ÇÁ=Ïb^Â”±æ,2+õ¥´4V”K:6:q`«ğ?‰á	Ìr$«vT°ºİYædÖ6Ut¡l0B˜…z:»çjIPrN-TÍ?²UNÖqæ¹O)$•È|ÎBüc¨BIÅğ¨b¨ ı6h<H¸o*X?é’³Cp?`oÜÑT]ÿĞ‹{2EÓ4@&Éaæò çt+^Tú4© !©¬¥q» ğê(„ä­d#¢'»(æ#IÄD{ab{:ËÊ·ü³(Ó~›´Z–ÔM) ›„á¹”©r2;×Pîš¨ÏL‰ŞYj'opn:zoüÂµTUí gW~¬ã&fóo>¢_It¾—t}T~,8¯#b)†€Pä|áÇÉPÓÃW¹ÁrÁDÁ=#Ñbûáå óÊÔÔ%*ŒnN»÷vu8y÷üôìaM?‰r­‰Phäíêu1•&ie8¾E1÷,‹/À)Š"‹ ³‰/ò“(Š± 95{e( îR|ddSùã™+mï$i4FxóêUy#8NTEï‰Î1vqä&´ÈÆ(r-ï¥Fz22 ‚Ìñà2 é8Š(ğû ã (ƒwÀ&°ùgiĞU}îÉcu¯Ÿôq4-@ÒÔfe0Küy{3éíùtê"(ÁHÚô¥Ò ®3z,rÄş©«"@¨(Èjî¼TX^@ãLq¨h¤p{òÛ0Ñé"·âCÉ Àhddi!(Î"l¶ŒV`£íi|­O.àÑNĞücE$>e”ùÊxÎ|‹vå(Ë®‰1oÜC ‡%œ@ôñlÌÁĞºª6¨ê½,)1ø!°ŒUáÊs$ò´)2 ˆsh ¬üvSxå"ªI TH7ñÔY±gÄ4ˆîQNh™
1e(pºbc4•hB§(¨P%¥ã6÷l¬`À"Ÿ64ãòC,u\úæ	´l[ÔD0Ö:*2véªó„¦ñ†•}~ì¬£üD¶u	|Ö
00æôÎßµm`p°ç>ö®!osFÖGçÀ²Èøia”“.[FùNÍ
f~í $8rH=kM…#1U*³3¬?ësWFUìÏ¬È:ëîog˜cÂCä9r›¦WÕ˜v„_‘~RpTà«\Êå62mæóöÛ¦gã(ºñ #Õ8Ó(|YøwPÇ2Çr-ÀMI”šyĞ:İºS@‘Lz‰l)S‰1Pˆ
âÀZ°ÕdT/èFo@lv0.ÚÆP'î;&'|Ë\~k$Ô‡ 1¼\H1œ„
"@CiKe™Y|›Û™l“oÀóó^NTé(MÅj@YºÒÌö>˜_ÖF¦øŸVŠ¹KN-lLUï?¯2"«nàğúf]	€€¬éŒALã åŠ
.$´*ıŠ7W~ 9m™³íªÅfB¹r×í-ÖbmhOen—ÎÑ¿I,bˆĞiS]²oYOEï¡¬¬d]´áÙò2Ÿw~ËKL=‚ Ã@Bäã²j¬Õ[#ª•SGê;öFsmjsèÈ/…n]‡@àÀh?â]¾¹

„2Ä“TOpO|…¾à0èw—'Q{œ—¯dˆ'JR€$a˜†\K9äzì”/X:öbw#aÎ^İã+’Ig¼KS$!¥ hº’ø:Õi^*ÕîßâŞ>ÊOvUĞÚ<ªL eCnù~eßo||kİê2ost|/›€N7e1dµá•¹;şìÎ›EÖ.`Æü›¶Áx}P_(l°cÆ/¾µ6wf]‡úÙÚb¶jğ l”?6=BbÜ)±do~ŒoIgo+ÓuŞ®2óe]g2a¯tn}õÏ¡I4UÂ,ƒÊôN²úm%Z nœ¼N)Ä;eú{Ïœ<„W Îa/bî@VBL¡štü£wBjìnsèôt|Å°Áê­nMhM<zÀYŠ­ŒdÃL` .œúf‚,h»H7­—	 sOdEñIûÊÇŒ[¥B‚D$ª`ËGóaàcŸ¦'´°çÚÃÃZö¢ş .ºn<«oßô@FÖ¹Š?‰-¾f]«s[E¤1ãç7,ãÃˆôÃÎ Va<à*sZÀã#İÌxRÅpámøv³KkûW‡xpo½ò^lrÀ¼Ób=Rr˜O'?øz(øuÌÙ$H M,J4#¬YÙsbÚ¤§Pw yú¦y+66Ní,::€úÚåÒldoçmÙ÷¢Vé®Xg B)aãpõ8Â (3`¤ÍtøA„{¦! À l‘Ê™ƒ@¦œé€'GoX{7´ôäÛ3¹eÌgÄı4zF2L-b*ëQFTŸª®Ñ£º%îb¬èk ©Ç!ÃHL8rÍæzì"X8›
´Ò>ånsnbÄş7	;Äç|şèÀPJĞ×$é>¦a^sînà¢Bm4İ÷Ï 9`Ù¡P!qiÒ°¯«¬ dà ÉÈAùm*!ÌÑføÖğ–WRk©³-u3öx#>HÔrm7î
KE„t÷ÃE$Op Â$¹@„¢ 6cìÄôåÄŠH5½Çp§¸¦YÊbaƒâMÿ)çheÊ3+UMÑV8¢<£‰ï=4ƒiR!S¼ÕlÒŠØB£	Ö¥vĞÎ :Ñªƒ¨¤ğj1F´ézÊl’A9ıı*™ ¡YÉS3(l@¢¶s=^…5âo(M÷¡toèhqê®Ğìäïl°ø®* rvĞâOŠ²cíeÃµ#¥|Gèá1`8zã4<€l3©/apË§§:zˆBõtÕ+#^Óş€q<±¿ôue„ºÓK`îç¬ Â> ”
<¬ògÒúÌ©¦ŒRÄßN¬O+¯DuŒŠ3.lC|}uÑ¼eš¿m`ÍÄæ§(õ^f‰k@ÿ:X`?àŒlûdNk„=U‡$ •^¶;$(8Û^—_D£`vH}Ş ´SbBÀA]d|uËš{¿;ßdÉsFÙR&l'‰!õ!¬ö½{l`ş¡âÊ:¢Ğá¤Ó¼]±r%s5Æ†6Õt¥Â8•ì¹?I(²<çï®gPîtÜ~Oi‚¡ŠÜVçQqÄšdfã*£ßì@¡lÀDÑwpı‚cŒTT^6blú¡ÀEwÑGQÕÛ°¨Ê9™2Šsb"$c"/Oª6ÙX%lÿ7ä‰ÄglÍõîl¼]Fê´o0uéM÷puR¸GÏÿgl,EJhwâYÑMôdxUeš­qA=lY÷t`1ÕE©1ºW™y|Š/ÏƒiÖ`gùa345<ÂWŞoá‹k2ŒhTm‚øn¥D{ò#…ç¤_*ÿy³¢®Ì!ìŞg°dİB‹È¢z(‚„I,Şq8€üNu"ô)¿`Oá¡‰º£àäÁHï€ìô(dd€*Ñ÷q{• ¾¨,T-U^qúYHµ@HI„8\Úêô{|0«WN¹.7m73n:ÁñE†N¥úu¹g)ìãloáPNsÇÏ«*l¼iGÔ Hsïßùoõ8H[ÂRã`ãAcíÔ”\2¢Ëîâô õ‚JaxĞÛXhb¼£ï¿G8õecÅ¹lf2EêØA:UëiöSH·-}àK8¦@ÜÉîH3Z~{/~ĞJ×B¸Š³¸İdó²äNœ6iÓw!ŞfÓyÆäÚöÅIè „¢qn-"ÜEÚ’/_~xg^Oßn­SEÖò”#FlôíOTãÍï¾e6<¬¡w}7Z	çctr'í5ó†-HËBLUfc™?/¨ÅeEæˆge`¦Wm\m$#eT¿S‚÷®T¤¿5!/B7,èºítépÅmt$¥ènËÛ×à Ç@cíYq
Œ0™nKcO>åtºêÁêmÆT¦!÷PpdşÛUbZÏL­ß·Éãp}]Tny0ìúØ¯{¸òí.Ü qepN'
wß±xfæ"®6%ØÚ«Ã÷Sb¿%4@}2´‡íC•7/p‚z¢ˆîhbÒ u´ 7úá$C¾ÎìR†dÆáH=Hõî¶)´!Ä©yT0Wó
LıÇÈÌIAóTu·aAå†ê	ñå®ÆUêèV#ƒƒˆ %7O?Ä^¾,BqäæÛàzòKu5¼ş«eTmhá`®y“ª^Ê!Ñ¯Éìwxl÷(Éİ¸ì2D vıöıRK‹=#1=:q8+•GÁË¢#¢½¤fdkn è°Î…jÁ+40~4ˆ_EwRßÏsXWŒS!ê&1Ú´V&QízŸ¨@µàÄsfaÁú©Í­ûüyè¤N({z‰ sĞT9%KµÂ§¸?yùe8³Ã(hl¢ËQ°‚%`;2âä4w†%ÕDå{ã,òrşOxëåeäûr!&JÒ=,Ê@ÒğiÌftÔöæAnÓÆ2È4ş»Õr=@4$bS8mòª)åQkùs”uë•TWWCƒ#R
`vºóŒÊM2»º ÷Å`rç^èLäüğ.^ÓHûØDŸùKIÌRSÏöéá2RœCtïÄ™ìÿV8³îwO¤ö&ªW`ofk|
 oé·}*$²Ã$â !4‹}8qÌ*šbØ¨ƒóbv• Â„Í*UOOsŸÉ÷p¾Ô†n,½ÄtWhÅd q"v,.ŠğO °+%
|Ë0eó¸$$?.¡êé¸("	Ml>«0H>ôo‹exme;ŒÚ›ûì\“k×Uó[O¶ßÄUst³/èÎ‘d‰Í€È1:Zy‚1{6¨1I$hræx`Wåw8ë®.Xv÷ğKıæF$i\dû8ª¤·]ZÜæxÌDj'„•yòi;¦îï¥W`î™rŠ"o¡ƒÙ‚ ,3ÀhÄcz~!ª£j:Î sÆ¹õxö·q;$İÃkP]‚¤	£¡¬¨>®º¬œ!ãE
òğl!å22xÇ=œL1ê¡Ba{[”?»äš@xVolK§óÆ2ÙèÂöM ”¿ Ç	;ğh—Y¥<*c‚¤(ŞQ‰0+¡’ .3w}ÿô?4n@p`\pPD±1Ïf¥ír©kR]VõämNuïhÚåëôg0™êjeh)æ3w9z“Seà[#ú)XA71”*à×+¾Ÿ6DÕ¬D6h#2>/,`&*ƒÉ ¹¹ 9`M"lèëj&öëå<g£‡Zú[²‹Âä2t.uÓ´ôv*}ÁkNÌ.œü¯Ç(èQôp(l¸ª
=o0Ä#IaštJ£€,:rFgH{÷úá	Ú²jé'“$ vp¼UbNÙ%Uà¶ó|  nâd„æMkb¦P|#a‹c¤
¾	B†ºZäŞ»%\o×bCõí<ºßŒĞ/ŸÚÍMä5†+Enµ{
Ú7Ö$¾ú›å3±qY8:ëèº(Z@1ı•éO²Ó)†ç«Œ¢`0 d‘&EóÏ-iNpPs‹·ğf_g–Er´™'aª„D· ¡=*®ì"Å"£éZ ,¶/'¾làñ_zviå‘h¨fıR2ÓyahÖmO²A3¹éBb}¯¤Çò(!Š8Åhi]µSÑ4²I˜$¿:àÄƒpèF·OÁğsÓZv 	Àu_* xêËzİÌ ˆ§48ş
óïA #„„g¨Ì@Æ¦r¢n«€Š#`‹2RùytökÌÇ}]Ìªqæ×Aü%“o°&Dşrc"‰ı4Dò@k  6O2õiR˜lv®qÛAdë^°¥N¶æ3Y²¢¡:ä^©»g$z1ºewçQ¾)¤šFYyß/ÓdìºÂáa@	Ò(cÊ0.cıÛV©fêwš›o–2|™£È"Â9±-Y(ke»TÔnqxå¤Å±(º-E#  «	t?K|øÖkçœ]ÆÁLÃı6	•@_€€Øƒïî}òIO9œ)¢÷äë3(Æ9il}
3w¤«Ù£`ª›÷0{S!_Ã]©)ô‰QkkevÅ4"ò/DEeÉş7jGÉÆ~4xş1Z§ë­*>é¢K§·sÄçò/6@Ü(­òÂ1øõ¼ñÓùx#¸Wà[iâ%ª~k,³¤¿1!LRa–âŸ «ª±o—ø}oôn1øbR>ûÊy$=WzFĞsPãsüXÅèHE ¦f;ö}?C9Tz3)ëOj|Fv¹qåâu_#(—í>Æ!öÜá-3ğêg-]m¥ìœ¬Ş.ª#;²µ×n)uõ&¨‚BD>#<…§õzÃÚçt}A0Ÿí8÷®EÏ'¿ŸMujª4/¬o*{¦îUê¤	áÔÙMp‚m^€êäx÷zXv7Îieg"à¾Û,ı-Ú4–MŒdo¼ÊjòãÖÀ¥şttÒçD4çMqgì&k7(cÊQ E	!UBjåFfôn[—tƒEáöä&âHº$7QDc(%o1_;L¤‘Ö)N£c¤z9úØEüG“6Tg|rÒFÅë/P $Á3­aızÑ¹k©bevd¤hzá¡&ºLuÛSÇìğ|¶ëOgeA:€ïÜM<JL¼‚4¨©÷„‹.( hgÆb—3›*²G6ğf 3ü t*í‹=vGµ8jR›XÏ¡Ÿ:7n,õÒ<CfğöÆ0Aò9Z”–f“	°³X pÈU%!ãDK2Š¡,¨v$1™ã$¤c£²£¨Ğ&¦8s¾|fş{
Ù§Õk!ŠEÉt‰k
o!c“,R å¡é F°E#« cg-$0 tòäu`­8¨ &iø¨1 Ùd²D¼d§D`85Æé1e S%}$«¯|¨àv©S pº¹û­ä0eÖ<1bc÷ rÆrˆb <`ÀxÉ êbí$ÔpXL€ƒæ à¦v¡3B>`ç‰cDlFlcixJpÄv¡iTÈulOğ–Ù#øm_.\fH‚}nø Ãx à§Jğ t&÷"° è$¯@²C‘g0÷ñùP¬ )o“}ßa#-i<qP?hO …¯v† ¥UcEiŠbõçqï-óm/vºEH60hO~!“^}Ôuyk%¨UV2zu¦%eeï0¸ì"Ó2`P.wªŠ¿¼D0\àl¤÷Sks”ÑabTãt¯R7ªTº‚vR~}sæx‡`!-_aı±
¡C:8À9Uã2åèœ"n6OEeÎçl¥uÂ‚VºvÅTLk3ğiøss¨­÷`öÁçIolv„˜3 ½ş’Jsbcrs0á*`zPsò5şW+ ğH¿gÔmnöêc¡¤¶„64Ùnw¤ÕQ é+X4cµKu)Š®Cnf$Z²4ìUB8½K.)-ŞcÎoø/Ô<x9[?8<†v{‡gíå.(¸ax?9kWå¤ãÖš/¯/×>G!&~r4b¾­W4¸¡8XÅôçRm<LJ‰Ğeˆ êu‡XJn%Ê}eQñGö²
5XjB±÷C‚!`$Å/~e ØÕtŠjã/°ñ% pa2&ÌâàºC§|Ùp`g©d)eJ¯ój2>6xÊ
wšvIÌiâ0˜c¬ Áª5sb;yİ—h9e[Û$ÄerÚâlĞ8Ks§Mhu 0ô)!"¹Xõ&tné·H0íô:Sû2ÍqüÄt@pòe+ıQZ[°''HpzĞó×=w^şQøzÁíàçH3"õ xtÙ ]4\ 0 Òğ åz8}ˆïÂ&9'ë{®`uEÒN„N%A:’~6K0\¥q:cğv]£ëF«<Mnï¸*lár È}ŠZˆ¨•2º†¬—d7'Õ
Ï%äBLàsfFUnvLh,m¥/^Hç8rF —4Aö4hluqâŒ3Të+¦!9£>y2² ¦¨{>6ßnê7ò÷O:;vÆàb¤85CNXFiguZX>ï>1ˆ`=”40Q×®b7)AÒ3òh)hI¢Pi¥¢¶ªæå ì!'x_AEA‰¸§	Og^íñıÚNx‹W¬™¦3÷sl¹wIhHqª„ü5hâm0=ãq˜sÒÌZ{Ñ¸Š[Vzp"ò#Ì!,¬ Â¥,`ó&Qo–lÙ&%=ıi¶‘xe!Èv©t*&Â},«a]èís |n‡SA˜¶$±.ü@4-!&’8gøâ d¥up7nd–š.“W S;§¡Æm¨Ä­r,á¹I—eÁ-:Ãup^>#²úïsôœeõ•½eC×'¡ jp£N õ>y½ºZµ}GroÓfL?œ¤\dcåĞä›—v/·Àğ­-¶upùo*ÅIaO!­^*Û©ºNQU¯ ¥wtñyäe>q¦d\ğX~<(÷ù 	$ÚAêó0 &tá8Ğ"Hï&|s_› nqt¦lcé·¦?£zll¾İ@²Û.­	'ïP2?ã½æÓ"ø¥§èU-v$ÉQç§áa­wiªeä¥şwv\`»Qjm}a¡k`õtGæÅ+éÂIXÖá+v)pÿ
ójÒehöOÆ_t 'òÁxğl5~i?œus*"/£Wp Ëõôb„`,QÀå„’m*(Ë‰Ğ=pî_j+"ç`g‹1Ùvaôh’{* ÃŸ‹oˆºp:n ÆesaÃöpv»__óˆcQaûö\Tª©d¤²hÍaaQ¹`Tz÷§R#Î¨HÃs'!"×+ğ3®%à<Ëºº„Ïoeái«D7s*>É0Ík-fPwÊ4'ljÎNpøTxfQet5¦6ŸiPf[%Ï9VÖfİ„àá$4ô;CmDàl:jt•`+-xÆ!o`ptaPB{e=lmqè2}.:ÕrEó `VeI•s 8;Şh5ô ä”)¤u0E$øSPIúsp{|w¦PàQï]r(yÜRu[à%qúÊís1Še<8iú),×„d ô°ú9 4ë±¦"0¥&û(S rr;Iqe@JÃ|wÂ®u¿9Up¤0)-97o,@‚6N"¦äéÀÎ*$à8‚è¸ˆj 1±uc 27!ÍEíá	)$!W¿tx&~Y4sh9°…>3ŠqÙ'¬E {¸t2¡"EXã`qh@°¥ÁÜ%$ZFVdm*v­qRœiò÷.!rRÉgíb|ÿ³ Q*aİ\j¿Æ(3! yp*t*/ 5ukh* ä şH$z°ú3¡’pqÉu¨Iñ­2³S<yb´C6ekA‡±ğ®<L¤)ôònd‡#. Ç¥ai100)boMóc1Û­sí>¶´dLAtğNe*¹úO“” L$t{p2°‹‚½SrL f¸o3kM!p¹µ­6wræõº vj£î•0Ü¥Ó!d¦ZN4ıC£`kî&ëoûbrp^HSkz×ïn6»xqœ)`âø¦W2EO2;PÓJ^(è"ª`ZMf,=œ!OŞF¼"ÙêÃSÅaıï$r.4RrVo²7Ò0è¤õb‰X”xOgBZ¹OWtqPÊ0$¬f@»@Ikg³xğvE•¶Ç(f]5Aïˆ®rLP¦mâ¥µsZ®×ŞzOhkÔL]µt›ÀJ¸mkR¦~ei¸N19!dÌ¤—VpĞ)§1ØÜcá+ïT|1šQTfi”açÑÁ'("oé‚oËŸçvc5,§¬,ìä1Út$:tÑõ_ÖåH;Q*Jal¢¾;ì¶[m±t¤•’pÁ`$4!c¡B|2rX½;T6f¢æDd¾C.)læhi~?¨iàgüàäa—(påTÀ® yö&ìTmê9c\ÉÄ·ãpï|1Õ=!
F/û$VQµ±¯OG*«‚ƒ¯ ¤ù}0+Ì¡5šú„9Í(#8Ğœ"MJzmcyNrJãËéR5Avå|'È
|N¼ª /6>»ô!İåmaBå9@ÔæJ#û"u<yÑv/§FoºI+½nË0£È„,`5_¸ôNˆpÒicl$,Xl¬l`,è^„ÆY;ÎÚ³i£ÏmÙ5IœQc5õ÷*|` ¼g{*çuéxk½¡ºFQ!‡Fó$Qy0·3¿¡ëf„¥¯p aLsuĞifûëdHÊe™(·>|sv Hxåm/;¶"óm/W&r¬[ITk…cyûU #²}„$¯-Q Ç²e³` p Vş:  kİ)ó	hôÔşjk­ï¯R  jÖ!TT:¨~xı*`ªòÄ1ìX\[²B¥c$úi)oê3¸/…eÕtÄb{{:yš¥U3•æÉo<$œÍÉg gBã|Œ+ìœ.û5õ!pUPúÃ|åmaËoª	eçLRğClzw¢2*¥â9óÁ¸Ílã5‘ÅKl%uNCğb6?.ˆ!KY<i"h0²(ödkSç7ãÏO{«šf4#Ù/0ooäF>ü
ûlÃ Do“cHTJV¼Ó,²t@¸L{8F_{ùÑ£3d¾aMQıa~$µ--iòoLõÒ;e¿~++Õ0û¨E%EjÜj+w7Rû :a6â–"$uaq(L=Bk3Hã¯o!ÓrĞ`sˆ4lUóSÑÿ(V´-al¨2ğ+PK‚`u bh­L°&Ösô4õc+'W\Eœ$*1àFˆ‹ÂMÔ=ênBœ?¯¬63÷÷½^•K:6ğWu/ñ?ñæS²«~TŸ»Êæf–6Ip!l"D¸Åv[ó·iCTr>(í?¶v¤{ô1¹G,¥l—À~ÄB~c#ÃF}Åğbb$8œ}v`4A{j:Ú<ïò³3 ?è/Ø¥l~ÿP±Ï{6UU=H(I'cæb¨çd7\Ğø4Ñ!Tan­%q»ôıùx3„äılgâ§ÿ¦E¤gINdD`%kz81ï‹Ê¶½31‡Ñ~{tp–ÕGy ;–Ñ=ôƒ%r`1EPîšèÏZÉşœS	'o`N*º/ÎË7eÅí°U[o(,	¨færq2²Oy(¦WtuÄ|,0+"r)‡@Q$/î<LRR£W¹ÅSÑäI9+qâùá© óÆÅ”,L*Œon{-~u8i÷é´êaI?™zÿ/@`çá£u3<K%8¾z£u,G?ã=@"Îà³	%Z³¹‹2}7gíUX<cR%à`QzËı#>É‘$H´dèsºTX'<G|EkMÆ9š ~å&|L§+b,î¡B`3kìÕà3 a0¢+ğû&F)(³ƒrå|6´ù¥fir}î*C½,ŸçsT',`Òäs`yb}x[;©åit*(hÅÓĞ¥  e"3j- P,V¾#' Eøhègîñt^úDã~²ªH¦sNù xŞ0uik³âCÅ»¾Êhfei>)Î mäCä³=)|­#,dØ|%`$j?e´öOXxof:,I>1mÜSm'dã(	.ä¾«> fı-91ø«puÁÆó&4 ·(R3!0w4` ,t|e ©K0´`7QßI°GQ·©ënIFiĞ@9G(p¸èc4•hJbh(R%%«·²m`Ä3vá"E¨e|úÂ))dôe1”":°vì©Òó œòÅ’“tzOn- ü„¢pA\ğ
p0öäÏß-hàp°æ,ÿïqFO§à"ˆé)%”Ñ~!9añNÉ[	Fí t9c—L?i‘g11*¿;UÌ;ia}FoèC¬É÷j¶"cq‘sóãD]v»ªV×Œ]‘>r9 Mñ“YŠf19Æ²waÛã$÷ ¼s 3•¼Ó(|:yyßP†Ç  àI‘5Y>İqhÑN:‰,©C©%Sì†¢À~0ul®égg@Âg4.[æP'N;#¥lËa)$Ñ¦q‘.>|l0„@>fTCioi©=nšÓHìµ"hĞó÷VLPÍ,iÍkD8¬Â‰ö¼–F©‰ïR¹k|n°lUo;,òbŠÍne±z&u€ ,ëÉCa#;eU  2dk	½/vR~,)/š¹¬ª€fr¾ó—ç-Üpm>EpÜWNñÿÉià-äqsM¦ñIOGí¡|‹<fzí°çĞ<òà7»vëoD?JÓ@Räã2"èôt5z+Â•'ÂGê?kB|ø"shT¬~mäPa†YO·±MJ…tô’”,pm$¼àèÓ§&0Y’‡æ 5*R  aˆÄ	\Y9äzíÖH2än7)[Î~O##“HFüKq? £ä(h:¸%1 Åi^$=õîâò/ÊOuÊ$ªMtCeüïW~(%}h{İRû octl!, "’Dwï1m•åŸ½3îj¯ªa—/pÇôS´×11¸N²wÆ-¾´Vwf]ƒúèøz nà0l!?f8FğÜm7 opkIõuc+0Û³ö®wñnÀlVkN¶ulyW§ã7(%WÂ4ãÊÔ¼{sm%Xh&îOlŞ'iH¡;‰xäl›Î•$8n@/n TFJáCŸuø¡q	Bê æsä¶ptG²Åk8F^hM>rÀ]Ê5¯Ü`Åb	0ö:÷Ö'¬hºlw¤	 pGd@åÿâåœX0C¢D š`é óa`b¤  Ÿ0ÅRêVe²ö"j?Ïî_;oÛĞaD9Î=ˆ9¾gë#G¤¾ş3`áv&`C0ö#1¨ `a<è(¬%Pá%İÀtå`aoğµ;K>ûO–hÑo½}sü~P¼`c>ĞrO3?|:Jàu`ı$( # j % Ğcbº¤³§Pr)xĞ ù+&4Ì $0¡€zßãPd>fâm÷#>©¦}f B8Iw õxPÂ!‰0{b´ŠtùE„r¤' Ä61 ‘*D¢Ğ,å% gTy³0t`È3µ7¨gD½qğ2Y‰¥â(‹óTÿküÑ\áÛmkjeyjpE©Æ3ğK<0Íäøè#H¼‹0ğ"±dÿ:b…z'%ÄÆxè @U^Ğ,o®¼$qúhÁºNx4õ-ğo!9biÅUeskö7¬©¬`làÉÈ!ù,	*¡Ìñ)øÄ²¦vR·8*£55;"í < Ô"v$¿jNÅT|ş4ó‰EXdg1¬Â!¹t\X©dvcätöÑgÌ¾hîob§½ÎqXgtƒê{-ï+ie*+ÕIõ:2,(‹î}4«apS¼õmáîKÂ#	Ú¤eÙû:Ó,ëÃznñj1Â) ë{Êd) LfV¹ôïPI¾v2(HV¢r);„5ân
OöB]!6.hh+ì.ğşfeh øº/Bbg8bß–géçyJÑñ3çmNïå1p0~í!4€m;7©&gã‡¥ãx*(µ`Q‰Ê#\HRş
q$³?¼pm„º¾_N3fÇ®næ85Ô8¤puÒûL)¤ˆRMß„G;«#duœæ5n¤#<}	u
,aš÷maÅd„7©ñ&oa`=¶{Ğy;à¾lûd:#¤<T	 $•^;,ugJ—_F³ex/`qæV¨›­cbÂÀP_mpëk		»;ştÈ{F[c"d'±5!¬¼½YlDï1ïZ:¢Ğõ¡Z0¼y9r12µÂÇgõt-r0õI9¾H:£æ÷êdQÌtØ~M~¡vá!uà²äfGá:£à eèÄLäVpù
#ŒUdv¶b|ê3¬&DF°¤A•ù±i?P<2ª³`"ec`vÃªn]}ulßuíFndØı¬i´UtÊtM14M	M÷4B-SšcO ½Jl}ÇHb_uEâùñòtW)šmx`;$YõWq14W£3¾_=y1}ˆ+óƒmè–@3™e0?uÉuŞoá*#2‘lPl˜e7%Q{t!$çs'*t${3²ªÊ/:˜~Å¤eBwƒ\â:$‚ŒO¬Zg<¼½Nt+v‹ŸaYRKà‹ºgÀõâJÿ ìü/%` *ñwés• (<P/ VQö®`Z@YHĞ(x2êõ{|1n‚W.¹¬!ïò5AêRÂ2ñE¦Tb®hı¹guMâ	kå\Ngãìê*|(<A–BÄ jsæfõ¿ı<A[J#QcÁ1ÉäœE¢Ê[ç×dÖJaXğ¹dàdøóĞM³Óõ'Ç?Lã0%ùz‚Ar>R]ëm÷OŞH÷*uõC>&· ÔÍJJ1
</'!:ÄZ—¸²£8øeñ²àk:j‘# át2æìÊºÒIè"²¡n9 ÍDŸ·
e=)~p^O_.%aâ”#F ô­E<ãG)í~å4¨¡qe'Yçcd7¬5stClUf?%1-.¹ÁTäc%à¡7	mT} %9E:P¼‚÷ª”„ ;=!#@7%`"­lÙ¾]ÅşxÕAhvÎÛ]â°ÇÔB¥[ujÜv!òÿ:¥0zk÷êÍRt&&°p .ú›bòë,î
"·A·q]G?Px05íğp½Ú5òü9ì‚jt-*u^¹xæâ¹6%ˆ"« qAp¿$0@y2[4§èB‡7$ "{¢€ìtPR t´¡%ªkuc·LäVTÖãZ=	½ş<6/µ9´¡9T²2 ùÆè©X% `¶u!å Ê	Ñ1å0í Uæmv'Ç¶ã!HŠ%',?Å?¾$`zdæÿâØ}!¤~¾#4P`Iac¢ñ²àf_Š!u7‰åw8æ§(èˆ ˜l `„60ô—õ^é–¼5q81=WÉËƒ!¢ıld|Ehn ¨°Šz¡c>LòÿB6ˆ_upRÜÇ9 [Ÿ¬+9!ê>1R4Öv{¡kàp4ãäo&aú©h1ìî~ıx¬H)k"M$7Öv:/K5Ğ¥°y
ùex³Ä+ihrƒUƒeAqâä4×"5D {ó,fXsïzëôwdß>Sc&nö-lÎIÕñiì"`ÈTäQ~Óò3vÙqş»}p=B}eBS0hP î=äWkÙS ıó‘rGWËï3pvúÓÔÙo;[2 2ÿ%ræDîbøğ.~C"òPUŸiIJD^féùu
/%2WŞRe¬Ä™í¯hóçuo¥¢vèŞpEfk| %ïé·} ¬rÃgÄv¢!u/ĞqÜi®ÔRø¨ƒ÷c¡T!Šm*^/ã¿P§v¼ÔŒd ½dfG å'#µ"V4.cto ˜!µTuû2esyn·<ƒâm°-*	]lüá0Y:ôgƒeiee9Ú;[èN?kEQó›g<@U#d»/çË›pƒyÀqVRY*aÔ+{r©‚1)hRæØfWuw}ë­Pt{pAıâF0¨idú(k…·yjäaøŒ4c$ñ”z9â;z¤æü¥6uvşyğrŠg|‰‘Ô†N$- k C{Ş	º³JzÏ)‚1×tÖ§5k%ÎãcÄ9€d	¡4(_¤¹lÙf
ğqp'årzüxF½Œ8¡â¡ja}?±/òd(H8fn4d
·›Ö:JlgÖ A •~ Si3ñh·Y¯T.sƒşÍ¨ŞS‰69¹›†'³6Mo>f?$OIr`T\PD:!Ïg%iv)“ZsGıl=ne+jÒEëcyÂj Emb3c9h“Rî°[#Ú)Ya'sMŒEàFn;¼œ4Dpğvy346/,k¦:ç˜É´¿» qz\"lëïâ„öïã=§§‡VúÓb)Êä6ùnwá ôw/y« ìEìTıof0éSİZplmîur´D)Móœ}J^Èl9wf" @stqé‰ê3ê¨/Ã 1rq<|"NÈ©UìwñlcbNâä…vEksquU"gÏc¬J~S‡vXiØ±%n?`Aòí<ª_Ñ)ŸjMîñ«At½_¢zÚ7®$»º«Å2±b]8ºéÈz_Q9é×éi¾ó/ç'§cu²d(/E—wï-h¡ntP{Ó·8ÒTïBEs)ìÉ+`Vª„D· $_iÕìÎsñJ0éJB,–j%nàpù]nk¯•*(¹n}I2Ã{aK÷='²A39ï"j4­¤Æé3 !yÇht_¶SP4;IºE¿:åüŸóá^·Ï@úcÃ|¦ àw[(ixp‰jÍo€d §5èş.u¦Cà%¤¥góÌBF¦t²ië- `2Sû| ¾|‡_Ìª1?g×ª4“f0'fz3#2_}KtBúe  kdo¶ñ(VdÎ?ì9rÛFÔiZ1åB6âujƒ#ezd^Í34z‘ª07c¾i¤”Yq3]C¯“pä¾êga@ Â(Lcú0mc}ëg9kªËO8Jv™£Ì [¸|Noçó>Lvqt¥åÑ°m²%Fc”ì+-}/Ixj0p»çÌlFÃMÔ}f^&aØ‡ïnİâaM=œ1à×äén	ö±ieu
e3sŞÑ#`¦}6.%ã_©;æˆSË{g0×f7 a-DDgÉ¼zGÑÎ[1yı1F¦ã?/zå¢J£·{Ğ÷v/6 ßB½pò1øô¬Ññi¥¸#°t`^ıæ¥ê|!Lã$ny$ÚV¦}€«Š%1D–š_Ÿôê;úbRvwÎq$5t+fPû@&rsôÕ)kb%0·RK*æxz_9šD°|÷.©_JpOv­Òåæ%='|—./ÂaóüaBno½|}%ˆ,îË.ú'n»÷nV:(W44ôn¨ÖÖG~"<¥¥µn‡Úpm1Mq?ålõ¦Ä ï5?ìuò-*45¬m ?"{4®UŠ¤³¥ôiI_ÖŸk~
êä{÷zUPt³Ï-hAo$à¼
{0ı-Ú4hˆ d¬æ~bçö@íşttóãD0o
Itg3c?8rÎuiA¡U .“bfõnQ–d mcöâ²²k¸($_,a r¥-3[«däUŞyn·¥#j1ë‡GîESw^'ğtÒfÇÎ/P*tÁ3¬#}>™º{f'cfføz¬©n—º5Š-ûçìp|¶c!f gaWL8 ¯üo<NI´0Wè!å„«(±tçĞd%×2Ûn’D7tg/ˆ3ı%õ*²YÜÁ–=vK¿p–z~Xı!67êå 4"eaö	ä0p8Ş´”f“! °3Y!cˆU!1ûDršì|¬uŸsñ=ã7eg³Óâèt'…¸s´xbò) é¥Á#!ªEÁ                    key_func_other_arguments,
                                  ::tensorflow::InputList
                                  init_func_other_arguments,
                                  ::tensorflow::InputList
                                  reduce_func_other_arguments,
                                  ::tensorflow::InputList
                                  finalize_func_other_arguments, const
                                  NameAttrList& key_func, const NameAttrList&
                                  init_func, const NameAttrList& reduce_func,
                                  const NameAttrList& finalize_func, const
                                  DataTypeSlice& output_types, const
                                  gtl::ArraySlice<PartialTensorShape>&
                                  output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that computes a windowed group-by on `input_dataset`.
///
/// // TODO(mrry): Support non-int64 keys.
///
/// Args:
/// * scope: A Scope object
/// * key_func: A function mapping an element of `input_dataset`, concatenated
/// with `key_func_other_arguments` to a scalar value of type DT_INT64.
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalGroupByWindowDataset {
 public:
  ExperimentalGroupByWindowDataset(const ::tensorflow::Scope& scope,
                                 ::tensorflow::Input input_dataset,
                                 ::tensorflow::InputList
                                 key_func_other_arguments,
                                 ::tensorflow::InputList
                                 reduce_func_other_arguments,
                                 ::tensorflow::InputList
                                 window_size_func_other_arguments, const
                                 NameAttrList& key_func, const NameAttrList&
                                 reduce_func, const NameAttrList&
                                 window_size_func, const DataTypeSlice&
                                 output_types, const
                                 gtl::ArraySlice<PartialTensorShape>&
                                 output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that contains the elements of `input_dataset` ignoring errors.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalIgnoreErrorsDataset {
 public:
  /// Optional attribute setters for ExperimentalIgnoreErrorsDataset
  struct Attrs {
    /// Defaults to false
    TF_MUST_USE_RESULT Attrs LogWarning(bool x) {
      Attrs ret = *this;
      ret.log_warning_ = x;
      return ret;
    }

    bool log_warning_ = false;
  };
  ExperimentalIgnoreErrorsDataset(const ::tensorflow::Scope& scope,
                                ::tensorflow::Input input_dataset, const
                                DataTypeSlice& output_types, const
                                gtl::ArraySlice<PartialTensorShape>&
                                output_shapes);
  ExperimentalIgnoreErrorsDataset(const ::tensorflow::Scope& scope,
                                ::tensorflow::Input input_dataset, const
                                DataTypeSlice& output_types, const
                                gtl::ArraySlice<PartialTensorShape>&
                                output_shapes, const
                                ExperimentalIgnoreErrorsDataset::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs LogWarning(bool x) {
    return Attrs().LogWarning(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Returns the name of the device on which `resource` has been placed.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The device tensor.
class ExperimentalIteratorGetDevice {
 public:
  ExperimentalIteratorGetDevice(const ::tensorflow::Scope& scope,
                              ::tensorflow::Input resource);
  operator ::tensorflow::Output() const { return device; }
  operator ::tensorflow::Input() const { return device; }
  ::tensorflow::Node* node() const { return device.node(); }

  Operation operation;
  ::tensorflow::Output device;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalLMDBDataset {
 public:
  ExperimentalLMDBDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                        filenames, const DataTypeSlice& output_types, const
                        gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Records the latency of producing `input_dataset` elements in a StatsAggregator.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalLatencyStatsDataset {
 public:
  ExperimentalLatencyStatsDataset(const ::tensorflow::Scope& scope,
                                ::tensorflow::Input input_dataset,
                                ::tensorflow::Input tag, const DataTypeSlice&
                                output_types, const
                                gtl::ArraySlice<PartialTensorShape>&
                                output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that fuses mapping with batching.
///
/// Creates a dataset that applies `f` to the outputs of `input_dataset` and then
/// batches `batch_size` of them.
///
/// Unlike a "MapDataset", which applies `f` sequentially, this dataset invokes up
/// to `batch_size * num_parallel_batches` copies of `f` in parallel.
///
/// Args:
/// * scope: A Scope object
/// * input_dataset: A variant tensor representing the input dataset.
/// * other_arguments: A list of tensors, typically values that were captured when building a closure
/// for `f`.
/// * batch_size: A scalar representing the number of elements to accumulate in a
/// batch. It determines the number of concurrent invocations of `f` that process
/// elements from `input_dataset` in parallel.
/// * num_parallel_calls: A scalar representing the maximum number of parallel invocations of the `map_fn`
/// function. Applying the `map_fn` on consecutive input elements in parallel has
/// the potential to improve input pipeline throughput.
/// * drop_remainder: A scalar representing whether the last batch should be dropped in case its size
/// is smaller than desired.
/// * f: A function to apply to the outputs of `input_dataset`.
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalMapAndBatchDataset {
 public:
  /// Optional attribute setters for ExperimentalMapAndBatchDataset
  struct Attrs {
    /// Defaults to false
    TF_MUST_USE_RESULT Attrs PreserveCardinality(bool x) {
      Attrs ret = *this;
      ret.preserve_cardinality_ = x;
      return ret;
    }

    bool preserve_cardinality_ = false;
  };
  ExperimentalMapAndBatchDataset(const ::tensorflow::Scope& scope,
                               ::tensorflow::Input input_dataset,
                               ::tensorflow::InputList other_arguments,
                               ::tensorflow::Input batch_size,
                               ::tensorflow::Input num_parallel_calls,
                               ::tensorflow::Input drop_remainder, const
                               NameAttrList& f, const DataTypeSlice&
                               output_types, const
                               gtl::ArraySlice<PartialTensorShape>&
                               output_shapes);
  ExperimentalMapAndBatchDataset(const ::tensorflow::Scope& scope,
                               ::tensorflow::Input input_dataset,
                               ::tensorflow::InputList other_arguments,
                               ::tensorflow::Input batch_size,
                               ::tensorflow::Input num_parallel_calls,
                               ::tensorflow::Input drop_remainder, const
                               NameAttrList& f, const DataTypeSlice&
                               output_types, const
                               gtl::ArraySlice<PartialTensorShape>&
                               output_shapes, const
                               ExperimentalMapAndBatchDataset::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs PreserveCardinality(bool x) {
    return Attrs().PreserveCardinality(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that applies `f` to the outputs of `input_dataset`.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalMapDataset {
 public:
  /// Optional attribute setters for ExperimentalMapDataset
  struct Attrs {
    /// Defaults to true
    TF_MUST_USE_RESULT Attrs UseInterOpParallelism(bool x) {
      Attrs ret = *this;
      ret.use_inter_op_parallelism_ = x;
      return ret;
    }

    /// Defaults to false
    TF_MUST_USE_RESULT Attrs PreserveCardinality(bool x) {
      Attrs ret = *this;
      ret.preserve_cardinality_ = x;
      return ret;
    }

    bool use_inter_op_parallelism_ = true;
    bool preserve_cardinality_ = false;
  };
  ExperimentalMapDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                       input_dataset, ::tensorflow::InputList other_arguments,
                       const NameAttrList& f, const DataTypeSlice&
                       output_types, const gtl::ArraySlice<PartialTensorShape>&
                       output_shapes);
  ExperimentalMapDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                       input_dataset, ::tensorflow::InputList other_arguments,
                       const NameAttrList& f, const DataTypeSlice&
                       output_types, const gtl::ArraySlice<PartialTensorShape>&
                       output_shapes, const ExperimentalMapDataset::Attrs&
                       attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs UseInterOpParallelism(bool x) {
    return Attrs().UseInterOpParallelism(x);
  }
  static Attrs PreserveCardinality(bool x) {
    return Attrs().PreserveCardinality(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalMatchingFilesDataset {
 public:
  ExperimentalMatchingFilesDataset(const ::tensorflow::Scope& scope,
                                 ::tensorflow::Input patterns);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that overrides the maximum intra-op parallelism.
///
/// Args:
/// * scope: A Scope object
/// * max_intra_op_parallelism: Identifies the maximum intra-op parallelism to use.
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalMaxIntraOpParallelismDataset {
 public:
  ExperimentalMaxIntraOpParallelismDataset(const ::tensorflow::Scope& scope,
                                         ::tensorflow::Input input_dataset,
                                         ::tensorflow::Input
                                         max_intra_op_parallelism, const
                                         DataTypeSlice& output_types, const
                                         gtl::ArraySlice<PartialTensorShape>&
                                         output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalNonSerializableDataset {
 public:
  ExperimentalNonSerializableDataset(const ::tensorflow::Scope& scope,
                                   ::tensorflow::Input input_dataset, const
                                   DataTypeSlice& output_types, const
                                   gtl::ArraySlice<PartialTensorShape>&
                                   output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that applies `f` to the outputs of `input_dataset`.
///
/// The resulting dataset is similar to the `InterleaveDataset`, with the exception
/// that if retrieving the next value from a dataset would cause the requester to
/// block, it will skip that input dataset. This dataset is especially useful
/// when loading data from a variable-latency datastores (e.g. HDFS, GCS), as it
/// allows the training step to proceed so long as some data is available.
///
/// !! WARNING !! This dataset is not deterministic!
///
/// Args:
/// * scope: A Scope object
/// * f: A function mapping elements of `input_dataset`, concatenated with
/// `other_arguments`, to a Dataset variant that contains elements matching
/// `output_types` and `output_shapes`.
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalParallelInterleaveDataset {
 public:
  ExperimentalParallelInterleaveDataset(const ::tensorflow::Scope& scope,
                                      ::tensorflow::Input input_dataset,
                                      ::tensorflow::InputList other_arguments,
                                      ::tensorflow::Input cycle_length,
                                      ::tensorflow::Input block_length,
                                      ::tensorflow::Input sloppy,
                                      ::tensorflow::Input
                                      buffer_output_elements,
                                      ::tensorflow::Input
                                      prefetch_input_elements, const
                                      NameAttrList& f, const DataTypeSlice&
                                      output_types, const
                                      gtl::ArraySlice<PartialTensorShape>&
                                      output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Transforms `input_dataset` containing `Example` protos as vectors of DT_STRING into a dataset of `Tensor` or `SparseTensor` objects representing the parsed features.
///
/// Args:
/// * scope: A Scope object
/// * dense_defaults: A dict mapping string keys to `Tensor`s.
/// The keys of the dict must match the dense_keys of the feature.
/// * sparse_keys: A list of string keys in the examples features.
/// The results for these keys will be returned as `SparseTensor` objects.
/// * dense_keys: A list of Ndense string Tensors (scalars).
/// The keys expected in the Examples features associated with dense values.
/// * sparse_types: A list of `DTypes` of the same length as `sparse_keys`.
/// Only `tf.float32` (`FloatList`), `tf.int64` (`Int64List`),
/// and `tf.string` (`BytesList`) are supported.
/// * dense_shapes: List of tuples with the same length as `dense_keys`.
/// The shape of the data for each dense feature referenced by `dense_keys`.
/// Required for any input tensors identified by `dense_keys`.  Must be
/// either fully defined, or may contain an unknown first dimension.
/// An unknown first dimension means the feature is treated as having
/// a variable number of blocks, and the output shape along this dimension
/// is considered unknown at graph build time.  Padding is applied for
/// minibatch elements smaller than the maximum number of blocks for the
/// given feature along this dimension.
/// * output_types: The type list for the return values.
/// * output_shapes: The list of shapes being produced.
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalParseExampleDataset {
 public:
  /// Optional attribute setters for ExperimentalParseExampleDataset
  struct Attrs {
    /// Defaults to false
    TF_MUST_USE_RESULT Attrs Sloppy(bool x) {
      Attrs ret = *this;
      ret.sloppy_ = x;
      return ret;
    }

    bool sloppy_ = false;
  };
  ExperimentalParseExampleDataset(const ::tensorflow::Scope& scope,
                                ::tensorflow::Input input_dataset,
                                ::tensorflow::Input num_parallel_calls,
                                ::tensorflow::InputList dense_defaults, const
                                gtl::ArraySlice<::tensorflow::tstring>&
                                sparse_keys, const
                                gtl::ArraySlice<::tensorflow::tstring>&
                                dense_keys, const DataTypeSlice& sparse_types,
                                const gtl::ArraySlice<PartialTensorShape>&
                                dense_shapes, const DataTypeSlice&
                                output_types, const
                                gtl::ArraySlice<PartialTensorShape>&
                                output_shapes);
  ExperimentalParseExampleDataset(const ::tensorflow::Scope& scope,
                                ::tensorflow::Input input_dataset,
                                ::tensorflow::Input num_parallel_calls,
                                ::tensorflow::InputList dense_defaults, const
                                gtl::ArraySlice<::tensorflow::tstring>&
                                sparse_keys, const
                                gtl::ArraySlice<::tensorflow::tstring>&
                                dense_keys, const DataTypeSlice& sparse_types,
                                const gtl::ArraySlice<PartialTensorShape>&
                                dense_shapes, const DataTypeSlice&
                                output_types, const
                                gtl::ArraySlice<PartialTensorShape>&
                                output_shapes, const
                                ExperimentalParseExampleDataset::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs Sloppy(bool x) {
    return Attrs().Sloppy(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that uses a custom thread pool to compute `input_dataset`.
///
/// Args:
/// * scope: A Scope object
/// * num_threads: Identifies the number of threads to use for the private threadpool.
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalPrivateThreadPoolDataset {
 public:
  ExperimentalPrivateThreadPoolDataset(const ::tensorflow::Scope& scope,
                                     ::tensorflow::Input input_dataset,
                                     ::tensorflow::Input num_threads, const
                                     DataTypeSlice& output_types, const
                                     gtl::ArraySlice<PartialTensorShape>&
                                     output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a Dataset that returns pseudorandom numbers.
///
/// Args:
/// * scope: A Scope object
/// * seed: A scalar seed for the random number generator. If either seed or
/// seed2 is set to be non-zero, the random number generator is seeded
/// by the given seed.  Otherwise, a random seed is used.
/// * seed2: A second scalar seed to avoid seed collision.
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalRandomDataset {
 public:
  ExperimentalRandomDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                          seed, ::tensorflow::Input seed2, const DataTypeSlice&
                          output_types, const
                          gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that changes the batch size.
///
/// Creates a dataset that changes the batch size of the dataset to current batch
/// size // num_replicas.
///
/// Args:
/// * scope: A Scope object
/// * input_dataset: A variant tensor representing the input dataset.
/// * num_replicas: A scalar representing the number of replicas to distribute this batch across. As
/// a result of this transformation the current batch size would end up being
/// divided  by this parameter.
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalRebatchDataset {
 public:
  /// Optional attribute setters for ExperimentalRebatchDataset
  struct Attrs {
    /// Defaults to true
    TF_MUST_USE_RESULT Attrs UseFallback(bool x) {
      Attrs ret = *this;
      ret.use_fallback_ = x;
      return ret;
    }

    bool use_fallback_ = true;
  };
  ExperimentalRebatchDataset(const ::tensorflow::Scope& scope,
                           ::tensorflow::Input input_dataset,
                           ::tensorflow::Input num_replicas, const
                           DataTypeSlice& output_types, const
                           gtl::ArraySlice<PartialTensorShape>& output_shapes);
  ExperimentalRebatchDataset(const ::tensorflow::Scope& scope,
                           ::tensorflow::Input input_dataset,
                           ::tensorflow::Input num_replicas, const
                           DataTypeSlice& output_types, const
                           gtl::ArraySlice<PartialTensorShape>& output_shapes,
                           const ExperimentalRebatchDataset::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs UseFallback(bool x) {
    return Attrs().UseFallback(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset successively reduces `f` over the elements of `input_dataset`.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalScanDataset {
 public:
  /// Optional attribute setters for ExperimentalScanDataset
  struct Attrs {
    /// Defaults to false
    TF_MUST_USE_RESULT Attrs PreserveCardinality(bool x) {
      Attrs ret = *this;
      ret.preserve_cardinality_ = x;
      return ret;
    }

    bool preserve_cardinality_ = false;
  };
  ExperimentalScanDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                        input_dataset, ::tensorflow::InputList initial_state,
                        ::tensorflow::InputList other_arguments, const
                        NameAttrList& f, const DataTypeSlice& output_types,
                        const gtl::ArraySlice<PartialTensorShape>&
                        output_shapes);
  ExperimentalScanDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                        input_dataset, ::tensorflow::InputList initial_state,
                        ::tensorflow::InputList other_arguments, const
                        NameAttrList& f, const DataTypeSlice& output_types,
                        const gtl::ArraySlice<PartialTensorShape>&
                        output_shapes, const ExperimentalScanDataset::Attrs&
                        attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs PreserveCardinality(bool x) {
    return Attrs().PreserveCardinality(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalSetStatsAggregatorDataset {
 public:
  ExperimentalSetStatsAggregatorDataset(const ::tensorflow::Scope& scope,
                                      ::tensorflow::Input input_dataset,
                                      ::tensorflow::Input stats_aggregator,
                                      ::tensorflow::Input tag,
                                      ::tensorflow::Input counter_prefix, const
                                      DataTypeSlice& output_types, const
                                      gtl::ArraySlice<PartialTensorShape>&
                                      output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalSleepDataset {
 public:
  ExperimentalSleepDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                         input_dataset, ::tensorflow::Input sleep_microseconds,
                         const DataTypeSlice& output_types, const
                         gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that passes a sliding window over `input_dataset`.
///
/// Args:
/// * scope: A Scope object
/// * window_size: A scalar representing the number of elements in the
/// sliding window.
/// * window_shift: A scalar representing the steps moving the sliding window
/// forward in one iteration. It must be positive.
/// * window_stride: A scalar representing the stride of the input elements of the sliding window.
/// It must be positive.
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalSlidingWindowDataset {
 public:
  ExperimentalSlidingWindowDataset(const ::tensorflow::Scope& scope,
                                 ::tensorflow::Input input_dataset,
                                 ::tensorflow::Input window_size,
                                 ::tensorflow::Input window_shift,
                                 ::tensorflow::Input window_stride, const
                                 DataTypeSlice& output_types, const
                                 gtl::ArraySlice<PartialTensorShape>&
                                 output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that executes a SQL query and emits rows of the result set.
///
/// Args:
/// * scope: A Scope object
/// * driver_name: The database type. Currently, the only supported type is 'sqlite'.
/// * data_source_name: A connection string to connect to the database.
/// * query: A SQL query to execute.
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalSqlDataset {
 public:
  ExperimentalSqlDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                       driver_name, ::tensorflow::Input data_source_name,
                       ::tensorflow::Input query, const DataTypeSlice&
                       output_types, const gtl::ArraySlice<PartialTensorShape>&
                       output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a statistics manager resource.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalStatsAggregatorHandle {
 public:
  /// Optional attribute setters for ExperimentalStatsAggregatorHandle
  struct Attrs {
    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs Container(StringPiece x) {
      Attrs ret = *this;
      ret.container_ = x;
      return ret;
    }

    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs SharedName(StringPiece x) {
      Attrs ret = *this;
      ret.shared_name_ = x;
      return ret;
    }

    StringPiece container_ = "";
    StringPiece shared_name_ = "";
  };
  ExperimentalStatsAggregatorHandle(const ::tensorflow::Scope& scope);
  ExperimentalStatsAggregatorHandle(const ::tensorflow::Scope& scope, const
                                  ExperimentalStatsAggregatorHandle::Attrs&
                                  attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs Container(StringPiece x) {
    return Attrs().Container(x);
  }
  static Attrs SharedName(StringPiece x) {
    return Attrs().SharedName(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Produces a summary of any statistics recorded by the given statistics manager.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The summary tensor.
class ExperimentalStatsAggregatorSummary {
 public:
  ExperimentalStatsAggregatorSummary(const ::tensorflow::Scope& scope,
                                   ::tensorflow::Input iterator);
  operator ::tensorflow::Output() const { return summary; }
  operator ::tensorflow::Input() const { return summary; }
  ::tensorflow::Node* node() const { return summary.node(); }

  Operation operation;
  ::tensorflow::Output summary;
};

/// Creates a dataset that stops iteration when predicate` is false.
///
/// The `predicate` function must return a scalar boolean and accept the
/// following arguments:
///
/// * One tensor for each component of an element of `input_dataset`.
/// * One tensor for each value in `other_arguments`.
///
/// Args:
/// * scope: A Scope object
/// * other_arguments: A list of tensors, typically values that were captured when
/// building a closure for `predicate`.
/// * predicate: A function returning a scalar boolean.
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalTakeWhileDataset {
 public:
  ExperimentalTakeWhileDataset(const ::tensorflow::Scope& scope,
                             ::tensorflow::Input input_dataset,
                             ::tensorflow::InputList other_arguments, const
                             NameAttrList& predicate, const DataTypeSlice&
                             output_types, const
                             gtl::ArraySlice<PartialTensorShape>&
                             output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that uses a custom thread pool to compute `input_dataset`.
///
/// Args:
/// * scope: A Scope object
/// * thread_pool: A resource produced by the ThreadPoolHandle op.
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalThreadPoolDataset {
 public:
  ExperimentalThreadPoolDataset(const ::tensorflow::Scope& scope,
                              ::tensorflow::Input input_dataset,
                              ::tensorflow::Input thread_pool, const
                              DataTypeSlice& output_types, const
                              gtl::ArraySlice<PartialTensorShape>&
                              output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that uses a custom thread pool to compute `input_dataset`.
///
/// Args:
/// * scope: A Scope object
/// * num_threads: The number of threads in the thread pool.
/// * display_name: A human-readable name for the threads that may be visible in some
/// visualizations.
/// threadpool.
///
/// Optional attributes (see `Attrs`):
/// * max_intra_op_parallelism: The maximum degree of parallelism to use within operations that execute on this
/// threadpool.
///
/// Returns:
/// * `Output`: A resource that can be consumed by one or more ExperimentalThreadPoolDataset
/// ops.
class ExperimentalThreadPoolHandle {
 public:
  /// Optional attribute setters for ExperimentalThreadPoolHandle
  struct Attrs {
    /// The maximum degree of parallelism to use within operations that execute on this
    /// threadpool.
    ///
    /// Defaults to 1
    TF_MUST_USE_RESULT Attrs MaxIntraOpParallelism(int64 x) {
      Attrs ret = *this;
      ret.max_intra_op_parallelism_ = x;
      return ret;
    }

    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs Container(StringPiece x) {
      Attrs ret = *this;
      ret.container_ = x;
      return ret;
    }

    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs SharedName(StringPiece x) {
      Attrs ret = *this;
      ret.shared_name_ = x;
      return ret;
    }

    int64 max_intra_op_parallelism_ = 1;
    StringPiece container_ = "";
    StringPiece shared_name_ = "";
  };
  ExperimentalThreadPoolHandle(const ::tensorflow::Scope& scope, int64
                             num_threads, StringPiece display_name);
  ExperimentalThreadPoolHandle(const ::tensorflow::Scope& scope, int64
                             num_threads, StringPiece display_name, const
                             ExperimentalThreadPoolHandle::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs MaxIntraOpParallelism(int64 x) {
    return Attrs().MaxIntraOpParallelism(x);
  }
  static Attrs Container(StringPiece x) {
    return Attrs().Container(x);
  }
  static Attrs SharedName(StringPiece x) {
    return Attrs().SharedName(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// A dataset that splits the elements of its input into multiple elements.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalUnbatchDataset {
 public:
  ExperimentalUnbatchDataset(const ::tensorflow::Scope& scope,
                           ::tensorflow::Input input_dataset, const
                           DataTypeSlice& output_types, const
                           gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that contains the unique elements of `input_dataset`.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class ExperimentalUniqueDataset {
 public:
  ExperimentalUniqueDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                          input_dataset, const DataTypeSlice& output_types,
                          const gtl::ArraySlice<PartialTensorShape>&
                          output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that computes a group-by on `input_dataset`.
///
/// Creates a dataset that computes a group-by on `input_dataset`.
///
/// Args:
/// * scope: A Scope object
/// * input_dataset: A variant tensor representing the input dataset.
/// * key_func_other_arguments: A list of tensors, typically values that were captured when
/// building a closure for `key_func`.
/// * init_func_other_arguments: A list of tensors, typically values that were captured when
/// building a closure for `init_func`.
/// * reduce_func_other_arguments: A list of tensors, typically values that were captured when
/// building a closure for `reduce_func`.
/// * finalize_func_other_arguments: A list of tensors, typically values that were captured when
/// building a closure for `finalize_func`.
/// * key_func: A function mapping an element of `input_dataset`, concatenated
/// with `key_func_other_arguments` to a scalar value of type DT_INT64.
/// * init_func: A function mapping a key of type DT_INT64, concatenated with
/// `init_func_other_arguments` to the initial reducer state.
/// * reduce_func: A function mapping the current reducer state and an element of `input_dataset`,
/// concatenated with `reduce_func_other_arguments` to a new reducer state.
/// * finalize_func: A function mapping the final reducer state to an output element.
///
/// Returns:
/// * `Output`: The handle tensor.
class GroupByReducerDataset {
 public:
  GroupByReducerDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                      input_dataset, ::tensorflow::InputList
                      key_func_other_arguments, ::tensorflow::InputList
                      init_func_other_arguments, ::tensorflow::InputList
                      reduce_func_other_arguments, ::tensorflow::InputList
                      finalize_func_other_arguments, const NameAttrList&
                      key_func, const NameAttrList& init_func, const
                      NameAttrList& reduce_func, const NameAttrList&
                      finalize_func, const DataTypeSlice& output_types, const
                      gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that computes a windowed group-by on `input_dataset`.
///
/// // TODO(mrry): Support non-int64 keys.
///
/// Args:
/// * scope: A Scope object
/// * key_func: A function mapping an element of `input_dataset`, concatenated
/// with `key_func_other_arguments` to a scalar value of type DT_INT64.
///
/// Returns:
/// * `Output`: The handle tensor.
class GroupByWindowDataset {
 public:
  GroupByWindowDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                     input_dataset, ::tensorflow::InputList
                     key_func_other_arguments, ::tensorflow::InputList
                     reduce_func_other_arguments, ::tensorflow::InputList
                     window_size_func_other_arguments, const NameAttrList&
                     key_func, const NameAttrList& reduce_func, const
                     NameAttrList& window_size_func, const DataTypeSlice&
                     output_types, const gtl::ArraySlice<PartialTensorShape>&
                     output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that contains the elements of `input_dataset` ignoring errors.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class IgnoreErrorsDataset {
 public:
  /// Optional attribute setters for IgnoreErrorsDataset
  struct Attrs {
    /// Defaults to false
    TF_MUST_USE_RESULT Attrs LogWarning(bool x) {
      Attrs ret = *this;
      ret.log_warning_ = x;
      return ret;
    }

    bool log_warning_ = false;
  };
  IgnoreErrorsDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                    input_dataset, const DataTypeSlice& output_types, const
                    gtl::ArraySlice<PartialTensorShape>& output_shapes);
  IgnoreErrorsDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                    input_dataset, const DataTypeSlice& output_types, const
                    gtl::ArraySlice<PartialTensorShape>& output_shapes, const
                    IgnoreErrorsDataset::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs LogWarning(bool x) {
    return Attrs().LogWarning(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * the created `Operation`
class InitializeTableFromDataset {
 public:
  InitializeTableFromDataset(const ::tensorflow::Scope& scope,
                           ::tensorflow::Input table_handle,
                           ::tensorflow::Input dataset);
  operator ::tensorflow::Operation() const { return operation; }

  Operation operation;
};

/// Returns the name of the device on which `resource` has been placed.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The device tensor.
class IteratorGetDevice {
 public:
  IteratorGetDevice(const ::tensorflow::Scope& scope, ::tensorflow::Input
                  resource);
  operator ::tensorflow::Output() const { return device; }
  operator ::tensorflow::Input() const { return device; }
  ::tensorflow::Node* node() const { return device.node(); }

  Operation operation;
  ::tensorflow::Output device;
};

/// Creates a dataset that emits the key-value pairs in one or more LMDB files.
///
/// The Lightning Memory-Mapped Database Manager, or LMDB, is an embedded binary
/// key-value database. This dataset can read the contents of LMDB database files,
/// the names of which generally have the `.mdb` suffix.
///
/// Each output element consists of a key-value pair represented as a pair of
/// scalar string `Tensor`s, where the first `Tensor` contains the key and the
/// second `Tensor` contains the value.
///
/// LMDB uses different file formats on big- and little-endian machines.
/// `LMDBDataset` can only read files in the format of the host machine.
///
/// Args:
/// * scope: A Scope object
/// * filenames: A scalar or a vector containing the name(s) of the binary file(s) to be
/// read.
///
/// Returns:
/// * `Output`: The handle tensor.
class LMDBDataset {
 public:
  LMDBDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input filenames,
            const DataTypeSlice& output_types, const
            gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Records the latency of producing `input_dataset` elements in a StatsAggregator.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class LatencyStatsDataset {
 public:
  LatencyStatsDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                    input_dataset, ::tensorflow::Input tag, const
                    DataTypeSlice& output_types, const
                    gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that applies `f` to the outputs of `input_dataset`.
///
/// The resulting dataset is similar to the `InterleaveDataset`, with the exception
/// that if retrieving the next value from a dataset would cause the requester to
/// block, it will skip that input dataset. This dataset is especially useful
/// when loading data from a variable-latency datastores (e.g. HDFS, GCS), as it
/// allows the training step to proceed so long as some data is available.
///
/// !! WARNING !! This dataset is not deterministic!
///
/// Args:
/// * scope: A Scope object
/// * f: A function mapping elements of `input_dataset`, concatenated with
/// `other_arguments`, to a Dataset variant that contains elements matching
/// `output_types` and `output_shapes`.
///
/// Returns:
/// * `Output`: The handle tensor.
class LegacyParallelInterleaveDatasetV2 {
 public:
  /// Optional attribute setters for LegacyParallelInterleaveDatasetV2
  struct Attrs {
    /// Defaults to "default"
    TF_MUST_USE_RESULT Attrs Deterministic(StringPiece x) {
      Attrs ret = *this;
      ret.deterministic_ = x;
      return ret;
    }

    StringPiece deterministic_ = "default";
  };
  LegacyParallelInterleaveDatasetV2(const ::tensorflow::Scope& scope,
                                  ::tensorflow::Input input_dataset,
                                  ::tensorflow::InputList other_arguments,
                                  ::tensorflow::Input cycle_length,
                                  ::tensorflow::Input block_length,
                                  ::tensorflow::Input buffer_output_elements,
                                  ::tensorflow::Input prefetch_input_elements,
                                  const NameAttrList& f, const DataTypeSlice&
                                  output_types, const
                                  gtl::ArraySlice<PartialTensorShape>&
                                  output_shapes);
  LegacyParallelInterleaveDatasetV2(const ::tensorflow::Scope& scope,
                                  ::tensorflow::Input input_dataset,
                                  ::tensorflow::InputList other_arguments,
                                  ::tensorflow::Input cycle_length,
                                  ::tensorflow::Input block_length,
                                  ::tensorflow::Input buffer_output_elements,
                                  ::tensorflow::Input prefetch_input_elements,
                                  const NameAttrList& f, const DataTypeSlice&
                                  output_types, const
                                  gtl::ArraySlice<PartialTensorShape>&
                                  output_shapes, const
                                  LegacyParallelInterleaveDatasetV2::Attrs&
                                  attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs Deterministic(StringPiece x) {
    return Attrs().Deterministic(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class LoadDataset {
 public:
  /// Optional attribute setters for LoadDataset
  struct Attrs {
    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs Compression(StringPiece x) {
      Attrs ret = *this;
      ret.compression_ = x;
      return ret;
    }

    StringPiece compression_ = "";
  };
  LoadDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input path,
            ::tensorflow::InputList reader_func_other_args, const
            DataTypeSlice& output_types, const
            gtl::ArraySlice<PartialTensorShape>& output_shapes, const
            NameAttrList& reader_func);
  LoadDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input path,
            ::tensorflow::InputList reader_func_other_args, const
            DataTypeSlice& output_types, const
            gtl::ArraySlice<PartialTensorShape>& output_shapes, const
            NameAttrList& reader_func, const LoadDataset::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs Compression(StringPiece x) {
    return Attrs().Compression(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that fuses mapping with batching.
///
/// Creates a dataset that applies `f` to the outputs of `input_dataset` and then
/// batches `batch_size` of them.
///
/// Unlike a "MapDataset", which applies `f` sequentially, this dataset invokes up
/// to `batch_size * num_parallel_batches` copies of `f` in parallel.
///
/// Args:
/// * scope: A Scope object
/// * input_dataset: A variant tensor representing the input dataset.
/// * other_arguments: A list of tensors, typically values that were captured when building a closure
/// for `f`.
/// * batch_size: A scalar representing the number of elements to accumulate in a
/// batch. It determines the number of concurrent invocations of `f` that process
/// elements from `input_dataset` in parallel.
/// * num_parallel_calls: A scalar representing the maximum number of parallel invocations of the `map_fn`
/// function. Applying the `map_fn` on consecutive input elements in parallel has
/// the potential to improve input pipeline throughput.
/// * drop_remainder: A scalar representing whether the last batch should be dropped in case its size
/// is smaller than desired.
/// * f: A function to apply to the outputs of `input_dataset`.
///
/// Returns:
/// * `Output`: The handle tensor.
class MapAndBatchDataset {
 public:
  /// Optional attribute setters for MapAndBatchDataset
  struct Attrs {
    /// Defaults to false
    TF_MUST_USE_RESULT Attrs PreserveCardinality(bool x) {
      Attrs ret = *this;
      ret.preserve_cardinality_ = x;
      return ret;
    }

    bool preserve_cardinality_ = false;
  };
  MapAndBatchDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                   input_dataset, ::tensorflow::InputList other_arguments,
                   ::tensorflow::Input batch_size, ::tensorflow::Input
                   num_parallel_calls, ::tensorflow::Input drop_remainder,
                   const NameAttrList& f, const DataTypeSlice& output_types,
                   const gtl::ArraySlice<PartialTensorShape>& output_shapes);
  MapAndBatchDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                   input_dataset, ::tensorflow::InputList other_arguments,
                   ::tensorflow::Input batch_size, ::tensorflow::Input
                   num_parallel_calls, ::tensorflow::Input drop_remainder,
                   const NameAttrList& f, const DataTypeSlice& output_types,
                   const gtl::ArraySlice<PartialTensorShape>& output_shapes,
                   const MapAndBatchDataset::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs PreserveCardinality(bool x) {
    return Attrs().PreserveCardinality(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class MatchingFilesDataset {
 public:
  MatchingFilesDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                     patterns);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that overrides the maximum intra-op parallelism.
///
/// Args:
/// * scope: A Scope object
/// * max_intra_op_parallelism: Identifies the maximum intra-op parallelism to use.
///
/// Returns:
/// * `Output`: The handle tensor.
class MaxIntraOpParallelismDataset {
 public:
  MaxIntraOpParallelismDataset(const ::tensorflow::Scope& scope,
                             ::tensorflow::Input input_dataset,
                             ::tensorflow::Input max_intra_op_parallelism,
                             const DataTypeSlice& output_types, const
                             gtl::ArraySlice<PartialTensorShape>&
                             output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class NonSerializableDataset {
 public:
  NonSerializableDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                       input_dataset, const DataTypeSlice& output_types, const
                       gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that applies `f` to the outputs of `input_dataset`.
///
/// The resulting dataset is similar to the `InterleaveDataset`, with the exception
/// that if retrieving the next value from a dataset would cause the requester to
/// block, it will skip that input dataset. This dataset is especially useful
/// when loading data from a variable-latency datastores (e.g. HDFS, GCS), as it
/// allows the training step to proceed so long as some data is available.
///
/// !! WARNING !! If the `sloppy` parameter is set to `True`, the operation of this
/// dataset will not be deterministic!
///
/// This dataset has been superseded by `ParallelInterleaveDatasetV2`.  New code
/// should use `ParallelInterleaveDatasetV2`.
///
/// The Python API `tf.data.experimental.parallel_interleave` creates instances of
/// this op. `tf.data.experimental.parallel_interleave` is a deprecated API.
///
/// Args:
/// * scope: A Scope object
/// * input_dataset: Dataset that produces a stream of arguments for the function `f`.
/// * other_arguments: Additional arguments to pass to `f` beyond those produced by `input_dataset`.
/// Evaluated once when the dataset is instantiated.
/// * cycle_length: Number of datasets (each created by applying `f` to the elements of
/// `input_dataset`) among which the `ParallelInterleaveDataset` will cycle in a
/// round-robin fashion.
/// * block_length: Number of elements at a time to produce from each interleaved invocation of a
/// dataset returned by `f`.
/// * sloppy: If `True`, return elements as they become available, even if that means returning
/// these elements in a non-deterministic order. Sloppy operation may result in better
/// performance in the presence of stragglers, but the dataset will still block if
/// all of its open streams are blocked.
/// If `False`, always return elements in a deterministic order.
/// * buffer_output_elements: The number of elements each iterator being interleaved should buffer (similar
/// to the `.prefetch()` transformation for each interleaved iterator).
/// * prefetch_input_elements: Determines the number of iterators to prefetch, allowing buffers to warm up and
/// data to be pre-fetched without blocking the main thread.
/// * f: A function mapping elements of `input_dataset`, concatenated with
/// `other_arguments`, to a Dataset variant that contains elements matching
/// `output_types` and `output_shapes`.
///
/// Returns:
/// * `Output`: The handle tensor.
class ParallelInterleaveDataset {
 public:
  ParallelInterleaveDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                          input_dataset, ::tensorflow::InputList
                          other_arguments, ::tensorflow::Input cycle_length,
                          ::tensorflow::Input block_length, ::tensorflow::Input
                          sloppy, ::tensorflow::Input buffer_output_elements,
                          ::tensorflow::Input prefetch_input_elements, const
                          NameAttrList& f, const DataTypeSlice& output_types,
                          const gtl::ArraySlice<PartialTensorShape>&
                          output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Transforms `input_dataset` containing `Example` protos as vectors of DT_STRING into a dataset of `Tensor` or `SparseTensor` objects representing the parsed features.
///
/// Args:
/// * scope: A Scope object
/// * dense_defaults: A dict mapping string keys to `Tensor`s.
/// The keys of the dict must match the dense_keys of the feature.
/// * sparse_keys: A list of string keys in the examples features.
/// The results for these keys will be returned as `SparseTensor` objects.
/// * dense_keys: A list of Ndense string Tensors (scalars).
/// The keys expected in the Examples features associated with dense values.
/// * sparse_types: A list of `DTypes` of the same length as `sparse_keys`.
/// Only `tf.float32` (`FloatList`), `tf.int64` (`Int64List`),
/// and `tf.string` (`BytesList`) are supported.
/// * dense_shapes: List of tuples with the same length as `dense_keys`.
/// The shape of the data for each dense feature referenced by `dense_keys`.
/// Required for any input tensors identified by `dense_keys`.  Must be
/// either fully defined, or may contain an unknown first dimension.
/// An unknown first dimension means the feature is treated as having
/// a variable number of blocks, and the output shape along this dimension
/// is considered unknown at graph build time.  Padding is applied for
/// minibatch elements smaller than the maximum number of blocks for the
/// given feature along this dimension.
/// * output_types: The type list for the return values.
/// * output_shapes: The list of shapes being produced.
///
/// Returns:
/// * `Output`: The handle tensor.
class ParseExampleDataset {
 public:
  /// Optional attribute setters for ParseExampleDataset
  struct Attrs {
    /// Defaults to false
    TF_MUST_USE_RESULT Attrs Sloppy(bool x) {
      Attrs ret = *this;
      ret.sloppy_ = x;
      return ret;
    }

    /// Defaults to []
    TF_MUST_USE_RESULT Attrs RaggedKeys(const gtl::ArraySlice<::tensorflow::tstring>& x) {
      Attrs ret = *this;
      ret.ragged_keys_ = x;
      return ret;
    }

    /// Defaults to []
    TF_MUST_USE_RESULT Attrs RaggedValueTypes(const DataTypeSlice& x) {
      Attrs ret = *this;
      ret.ragged_value_types_ = x;
      return ret;
    }

    /// Defaults to []
    TF_MUST_USE_RESULT Attrs RaggedSplitTypes(const DataTypeSlice& x) {
      Attrs ret = *this;
      ret.ragged_split_types_ = x;
      return ret;
    }

    bool sloppy_ = false;
    gtl::ArraySlice<::tensorflow::tstring> ragged_keys_ = {};
    DataTypeSlice ragged_value_types_ = {};
    DataTypeSlice ragged_split_types_ = {};
  };
  ParseExampleDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                    input_dataset, ::tensorflow::Input num_parallel_calls,
                    ::tensorflow::InputList dense_defaults, const
                    gtl::ArraySlice<::tensorflow::tstring>& sparse_keys, const
                    gtl::ArraySlice<::tensorflow::tstring>& dense_keys, const
                    DataTypeSlice& sparse_types, const
                    gtl::ArraySlice<PartialTensorShape>& dense_shapes, const
                    DataTypeSlice& output_types, const
                    gtl::ArraySlice<PartialTensorShape>& output_shapes);
  ParseExampleDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                    input_dataset, ::tensorflow::Input num_parallel_calls,
                    ::tensorflow::InputList dense_defaults, const
                    gtl::ArraySlice<::tensorflow::tstring>& sparse_keys, const
                    gtl::ArraySlice<::tensorflow::tstring>& dense_keys, const
                    DataTypeSlice& sparse_types, const
                    gtl::ArraySlice<PartialTensorShape>& dense_shapes, const
                    DataTypeSlice& output_types, const
                    gtl::ArraySlice<PartialTensorShape>& output_shapes, const
                    ParseExampleDataset::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs Sloppy(bool x) {
    return Attrs().Sloppy(x);
  }
  static Attrs RaggedKeys(const gtl::ArraySlice<::tensorflow::tstring>& x) {
    return Attrs().RaggedKeys(x);
  }
  static Attrs RaggedValueTypes(const DataTypeSlice& x) {
    return Attrs().RaggedValueTypes(x);
  }
  static Attrs RaggedSplitTypes(const DataTypeSlice& x) {
    return Attrs().RaggedSplitTypes(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Transforms `input_dataset` containing `Example` protos as vectors of DT_STRING into a dataset of `Tensor` or `SparseTensor` objects representing the parsed features.
///
/// Args:
/// * scope: A Scope object
/// * dense_defaults: A dict mapping string keys to `Tensor`s.
/// The keys of the dict must match the dense_keys of the feature.
/// * sparse_keys: A list of string keys in the examples features.
/// The results for these keys will be returned as `SparseTensor` objects.
/// * dense_keys: A list of Ndense string Tensors (scalars).
/// The keys expected in the Examples features associated with dense values.
/// * sparse_types: A list of `DTypes` of the same length as `sparse_keys`.
/// Only `tf.float32` (`FloatList`), `tf.int64` (`Int64List`),
/// and `tf.string` (`BytesList`) are supported.
/// * dense_shapes: List of tuples with the same length as `dense_keys`.
/// The shape of the data for each dense feature referenced by `dense_keys`.
/// Required for any input tensors identified by `dense_keys`.  Must be
/// either fully defined, or may contain an unknown first dimension.
/// An unknown first dimension means the feature is treated as having
/// a variable number of blocks, and the output shape along this dimension
/// is considered unknown at graph build time.  Padding is applied for
/// minibatch elements smaller than the maximum number of blocks for the
/// given feature along this dimension.
/// * output_types: The type list for the return values.
/// * output_shapes: The list of shapes being produced.
///
/// Optional attributes (see `Attrs`):
/// * deterministic: A string indicating the op-level determinism to use. Deterministic controls
/// whether the dataset is allowed to return elements out of order if the next
/// element to be returned isn't available, but a later element is. Options are
/// "true", "false", and "default". "default" indicates that determinism should be
/// decided by the `experimental_deterministic` parameter of `tf.data.Options`.
///
/// Returns:
/// * `Output`: The handle tensor.
class ParseExampleDatasetV2 {
 public:
  /// Optional attribute setters for ParseExampleDatasetV2
  struct Attrs {
    /// A string indicating the op-level determinism to use. Deterministic controls
    /// whether the dataset is allowed to return elements out of order if the next
    /// element to be returned isn't available, but a later element is. Options are
    /// "true", "false", and "default". "default" indicates that determinism should be
    /// decided by the `experimental_deterministic` parameter of `tf.data.Options`.
    ///
    /// Defaults to "default"
    TF_MUST_USE_RESULT Attrs Deterministic(StringPiece x) {
      Attrs ret = *this;
      ret.deterministic_ = x;
      return ret;
    }

    /// Defaults to []
    TF_MUST_USE_RESULT Attrs RaggedKeys(const gtl::ArraySlice<::tensorflow::tstring>& x) {
      Attrs ret = *this;
      ret.ragged_keys_ = x;
      return ret;
    }

    /// Defaults to []
    TF_MUST_USE_RESULT Attrs RaggedValueTypes(const DataTypeSlice& x) {
      Attrs ret = *this;
      ret.ragged_value_types_ = x;
      return ret;
    }

    /// Defaults to []
    TF_MUST_USE_RESULT Attrs RaggedSplitTypes(const DataTypeSlice& x) {
      Attrs ret = *this;
      ret.ragged_split_types_ = x;
      return ret;
    }

    StringPiece deterministic_ = "default";
    gtl::ArraySlice<::tensorflow::tstring> ragged_keys_ = {};
    DataTypeSlice ragged_value_types_ = {};
    DataTypeSlice ragged_split_types_ = {};
  };
  ParseExampleDatasetV2(const ::tensorflow::Scope& scope, ::tensorflow::Input
                      input_dataset, ::tensorflow::Input num_parallel_calls,
                      ::tensorflow::InputList dense_defaults, const
                      gtl::ArraySlice<::tensorflow::tstring>& sparse_keys,
                      const gtl::ArraySlice<::tensorflow::tstring>& dense_keys,
                      const DataTypeSlice& sparse_types, const
                      gtl::ArraySlice<PartialTensorShape>& dense_shapes, const
                      DataTypeSlice& output_types, const
                      gtl::ArraySlice<PartialTensorShape>& output_shapes);
  ParseExampleDatasetV2(const ::tensorflow::Scope& scope, ::tensorflow::Input
                      input_dataset, ::tensorflow::Input num_parallel_calls,
                      ::tensorflow::InputList dense_defaults, const
                      gtl::ArraySlice<::tensorflow::tstring>& sparse_keys,
                      const gtl::ArraySlice<::tensorflow::tstring>& dense_keys,
                      const DataTypeSlice& sparse_types, const
                      gtl::ArraySlice<PartialTensorShape>& dense_shapes, const
                      DataTypeSlice& output_types, const
                      gtl::ArraySlice<PartialTensorShape>& output_shapes, const
                      ParseExampleDatasetV2::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs Deterministic(StringPiece x) {
    return Attrs().Deterministic(x);
  }
  static Attrs RaggedKeys(const gtl::ArraySlice<::tensorflow::tstring>& x) {
    return Attrs().RaggedKeys(x);
  }
  static Attrs RaggedValueTypes(const DataTypeSlice& x) {
    return Attrs().RaggedValueTypes(x);
  }
  static Attrs RaggedSplitTypes(const DataTypeSlice& x) {
    return Attrs().RaggedSplitTypes(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that uses a custom thread pool to compute `input_dataset`.
///
/// Args:
/// * scope: A Scope object
/// * num_threads: Identifies the number of threads to use for the private threadpool.
///
/// Returns:
/// * `Output`: The handle tensor.
class PrivateThreadPoolDataset {
 public:
  PrivateThreadPoolDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                         input_dataset, ::tensorflow::Input num_threads, const
                         DataTypeSlice& output_types, const
                         gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a Dataset that returns pseudorandom numbers.
///
/// Creates a Dataset that returns a stream of uniformly distributed
/// pseudorandom 64-bit signed integers.
///
/// In the TensorFlow Python API, you can instantiate this dataset via the
/// class `tf.data.experimental.RandomDataset`.
///
/// Instances of this dataset are also created as a result of the
/// `hoist_random_uniform` static optimization. Whether this optimization is
/// performed is determined by the `experimental_optimization.hoist_random_uniform`
/// option of `tf.data.Options`.
///
/// Args:
/// * scope: A Scope object
/// * seed: A scalar seed for the random number generator. If either seed or
/// seed2 is set to be non-zero, the random number generator is seeded
/// by the given seed.  Otherwise, a random seed is used.
/// * seed2: A second scalar seed to avoid seed collision.
///
/// Returns:
/// * `Output`: The handle tensor.
class RandomDataset {
 public:
  RandomDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input seed,
              ::tensorflow::Input seed2, const DataTypeSlice& output_types,
              const gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that changes the batch size.
///
/// Creates a dataset that changes the batch size of the dataset to current batch
/// size // num_workers.
///
/// Args:
/// * scope: A Scope object
/// * input_dataset: A variant tensor representing the input dataset.
/// * num_replicas: A scalar representing the number of replicas to distribute this batch across. As
/// a result of this transformation the current batch size would end up being
/// divided  by this parameter.
///
/// Returns:
/// * `Output`: The handle tensor.
class RebatchDataset {
 public:
  /// Optional attribute setters for RebatchDataset
  struct Attrs {
    /// Defaults to true
    TF_MUST_USE_RESULT Attrs UseFallback(bool x) {
      Attrs ret = *this;
      ret.use_fallback_ = x;
      return ret;
    }

    bool use_fallback_ = true;
  };
  RebatchDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
               input_dataset, ::tensorflow::Input num_replicas, const
               DataTypeSlice& output_types, const
               gtl::ArraySlice<PartialTensorShape>& output_shapes);
  RebatchDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
               input_dataset, ::tensorflow::Input num_replicas, const
               DataTypeSlice& output_types, const
               gtl::ArraySlice<PartialTensorShape>& output_shapes, const
               RebatchDataset::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs UseFallback(bool x) {
    return Attrs().UseFallback(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that changes the batch size.
///
/// Creates a dataset that rebatches elements from `input_dataset` into new batch
/// sizes.
///
/// Args:
/// * scope: A Scope object
/// * input_dataset: A variant tensor representing the input dataset.
/// * batch_sizes: A vector of integers representing the size of batches to produce. These values
/// are cycled through in order.
///
/// Returns:
/// * `Output`: The handle tensor.
class RebatchDatasetV2 {
 public:
  RebatchDatasetV2(const ::tensorflow::Scope& scope, ::tensorflow::Input
                 input_dataset, ::tensorflow::Input batch_sizes,
                 ::tensorflow::Input drop_remainder, const DataTypeSlice&
                 output_types, const gtl::ArraySlice<PartialTensorShape>&
                 output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Registers a dataset with the tf.data service.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The dataset_id tensor.
class RegisterDataset {
 public:
  RegisterDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input dataset,
                ::tensorflow::Input address, ::tensorflow::Input protocol,
                int64 external_state_policy);
  operator ::tensorflow::Output() const { return dataset_id; }
  operator ::tensorflow::Input() const { return dataset_id; }
  ::tensorflow::Node* node() const { return dataset_id.node(); }

  Operation operation;
  ::tensorflow::Output dataset_id;
};

/// Creates a dataset that takes a Bernoulli sample of the contents of another dataset.
///
/// There is no transformation in the `tf.data` Python API for creating this dataset.
/// Instead, it is created as a result of the `filter_with_random_uniform_fusion`
/// static optimization. Whether this optimization is performed is determined by the
/// `experimental_optimization.filter_with_random_uniform_fusion` option of
/// `tf.data.Options`.
///
/// Args:
/// * scope: A Scope object
/// * rate: A scalar representing the sample rate. Each element of `input_dataset` is
/// retained with this probability, independent of all other elements.
/// * seed: A scalar representing seed of random number generator.
/// * seed2: A scalar representing seed2 of random number generator.
///
/// Returns:
/// * `Output`: The handle tensor.
class SamplingDataset {
 public:
  SamplingDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                input_dataset, ::tensorflow::Input rate, ::tensorflow::Input
                seed, ::tensorflow::Input seed2, const DataTypeSlice&
                output_types, const gtl::ArraySlice<PartialTensorShape>&
                output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * the created `Operation`
class SaveDataset {
 public:
  /// Optional attribute setters for SaveDataset
  struct Attrs {
    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs Compression(StringPiece x) {
      Attrs ret = *this;
      ret.compression_ = x;
      return ret;
    }

    /// Defaults to true
    TF_MUST_USE_RESULT Attrs UseShardFunc(bool x) {
      Attrs ret = *this;
      ret.use_shard_func_ = x;
      return ret;
    }

    StringPiece compression_ = "";
    bool use_shard_func_ = true;
  };
  SaveDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
            input_dataset, ::tensorflow::Input path, ::tensorflow::InputList
            shard_func_other_args, const NameAttrList& shard_func);
  SaveDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
            input_dataset, ::tensorflow::Input path, ::tensorflow::InputList
            shard_func_other_args, const NameAttrList& shard_func, const
            SaveDataset::Attrs& attrs);
  operator ::tensorflow::Operation() const { return operation; }

  static Attrs Compression(StringPiece x) {
    return Attrs().Compression(x);
  }
  static Attrs UseShardFunc(bool x) {
    return Attrs().UseShardFunc(x);
  }

  Operation operation;
};

/// Creates a dataset successively reduces `f` over the elements of `input_dataset`.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class ScanDataset {
 public:
  /// Optional attribute setters for ScanDataset
  struct Attrs {
    /// Defaults to false
    TF_MUST_USE_RESULT Attrs PreserveCardinality(bool x) {
      Attrs ret = *this;
      ret.preserve_cardinality_ = x;
      return ret;
    }

    /// Defaults to true
    TF_MUST_USE_RESULT Attrs UseDefaultDevice(bool x) {
      Attrs ret = *this;
      ret.use_default_device_ = x;
      return ret;
    }

    bool preserve_cardinality_ = false;
    bool use_default_device_ = true;
  };
  ScanDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
            input_dataset, ::tensorflow::InputList initial_state,
            ::tensorflow::InputList other_arguments, const NameAttrList& f,
            const DataTypeSlice& output_types, const
            gtl::ArraySlice<PartialTensorShape>& output_shapes);
  ScanDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
            input_dataset, ::tensorflow::InputList initial_state,
            ::tensorflow::InputList other_arguments, const NameAttrList& f,
            const DataTypeSlice& output_types, const
            gtl::ArraySlice<PartialTensorShape>& output_shapes, const
            ScanDataset::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs PreserveCardinality(bool x) {
    return Attrs().PreserveCardinality(x);
  }
  static Attrs UseDefaultDevice(bool x) {
    return Attrs().UseDefaultDevice(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class SetStatsAggregatorDataset {
 public:
  SetStatsAggregatorDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                          input_dataset, ::tensorflow::Input stats_aggregator,
                          ::tensorflow::Input tag, ::tensorflow::Input
                          counter_prefix, const DataTypeSlice& output_types,
                          const gtl::ArraySlice<PartialTensorShape>&
                          output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class SleepDataset {
 public:
  SleepDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
             input_dataset, ::tensorflow::Input sleep_microseconds, const
             DataTypeSlice& output_types, const
             gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that passes a sliding window over `input_dataset`.
///
/// Args:
/// * scope: A Scope object
/// * window_size: A scalar representing the number of elements in the
/// sliding window.
/// * window_shift: A scalar representing the steps moving the sliding window
/// forward in one iteration. It must be positive.
/// * window_stride: A scalar representing the stride of the input elements of the sliding window.
/// It must be positive.
///
/// Returns:
/// * `Output`: The handle tensor.
class SlidingWindowDataset {
 public:
  SlidingWindowDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                     input_dataset, ::tensorflow::Input window_size,
                     ::tensorflow::Input window_shift, ::tensorflow::Input
                     window_stride, const DataTypeSlice& output_types, const
                     gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that will write to / read from a snapshot.
///
/// This dataset attempts to determine whether a valid snapshot exists at the
/// `snapshot_path`, and reads from the snapshot in lieu of using `input_dataset`.
/// If not, it will run the preprocessing pipeline as usual, and write out a
/// snapshot of the data processed for future use.
///
/// Args:
/// * scope: A Scope object
/// * input_dataset: A variant tensor representing the input dataset.
/// * path: The path we should write snapshots to / read snapshots from.
///
/// Returns:
/// * `Output`: The handle tensor.
class SnapshotDataset {
 public:
  /// Optional attribute setters for SnapshotDataset
  struct Attrs {
    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs Compression(StringPiece x) {
      Attrs ret = *this;
      ret.compression_ = x;
      return ret;
    }

    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs ReaderPathPrefix(StringPiece x) {
      Attrs ret = *this;
      ret.reader_path_prefix_ = x;
      return ret;
    }

    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs WriterPathPrefix(StringPiece x) {
      Attrs ret = *this;
      ret.writer_path_prefix_ = x;
      return ret;
    }

    /// Defaults to 10737418240
    TF_MUST_USE_RESULT Attrs ShardSizeBytes(int64 x) {
      Attrs ret = *this;
      ret.shard_size_bytes_ = x;
      return ret;
    }

    /// Defaults to 86400
    TF_MUST_USE_RESULT Attrs PendingSnapshotExpirySeconds(int64 x) {
      Attrs ret = *this;
      ret.pending_snapshot_expiry_seconds_ = x;
      return ret;
    }

    /// Defaults to 1
    TF_MUST_USE_RESULT Attrs NumReaderThreads(int64 x) {
      Attrs ret = *this;
      ret.num_reader_threads_ = x;
      return ret;
    }

    /// Defaults to 1
    TF_MUST_USE_RESULT Attrs ReaderBufferSize(int64 x) {
      Attrs ret = *this;
      ret.reader_buffer_size_ = x;
      return ret;
    }

    /// Defaults to 1
    TF_MUST_USE_RESULT Attrs NumWriterThreads(int64 x) {
      Attrs ret = *this;
      ret.num_writer_threads_ = x;
      return ret;
    }

    /// Defaults to 1
    TF_MUST_USE_RESULT Attrs WriterBufferSize(int64 x) {
      Attrs ret = *this;
      ret.writer_buffer_size_ = x;
      return ret;
    }

    /// Defaults to false
    TF_MUST_USE_RESULT Attrs ShuffleOnRead(bool x) {
      Attrs ret = *this;
      ret.shuffle_on_read_ = x;
      return ret;
    }

    /// Defaults to 0
    TF_MUST_USE_RESULT Attrs Seed(int64 x) {
      Attrs ret = *this;
      ret.seed_ = x;
      return ret;
    }

    /// Defaults to 0
    TF_MUST_USE_RESULT Attrs Seed2(int64 x) {
      Attrs ret = *this;
      ret.seed2_ = x;
      return ret;
    }

    /// Defaults to "auto"
    TF_MUST_USE_RESULT Attrs Mode(StringPiece x) {
      Attrs ret = *this;
      ret.mode_ = x;
      return ret;
    }

    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs SnapshotName(StringPiece x) {
      Attrs ret = *this;
      ret.snapshot_name_ = x;
      return ret;
    }

    StringPiece compression_ = "";
    StringPiece reader_path_prefix_ = "";
    StringPiece writer_path_prefix_ = "";
    int64 shard_size_bytes_ = 10737418240;
    int64 pending_snapshot_expiry_seconds_ = 86400;
    int64 num_reader_threads_ = 1;
    int64 reader_buffer_size_ = 1;
    int64 num_writer_threads_ = 1;
    int64 writer_buffer_size_ = 1;
    bool shuffle_on_read_ = false;
    int64 seed_ = 0;
    int64 seed2_ = 0;
    StringPiece mode_ = "auto";
    StringPiece snapshot_name_ = "";
  };
  SnapshotDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                input_dataset, ::tensorflow::Input path, const DataTypeSlice&
                output_types, const gtl::ArraySlice<PartialTensorShape>&
                output_shapes);
  SnapshotDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                input_dataset, ::tensorflow::Input path, const DataTypeSlice&
                output_types, const gtl::ArraySlice<PartialTensorShape>&
                output_shapes, const SnapshotDataset::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs Compression(StringPiece x) {
    return Attrs().Compression(x);
  }
  static Attrs ReaderPathPrefix(StringPiece x) {
    return Attrs().ReaderPathPrefix(x);
  }
  static Attrs WriterPathPrefix(StringPiece x) {
    return Attrs().WriterPathPrefix(x);
  }
  static Attrs ShardSizeBytes(int64 x) {
    return Attrs().ShardSizeBytes(x);
  }
  static Attrs PendingSnapshotExpirySeconds(int64 x) {
    return Attrs().PendingSnapshotExpirySeconds(x);
  }
  static Attrs NumReaderThreads(int64 x) {
    return Attrs().NumReaderThreads(x);
  }
  static Attrs ReaderBufferSize(int64 x) {
    return Attrs().ReaderBufferSize(x);
  }
  static Attrs NumWriterThreads(int64 x) {
    return Attrs().NumWriterThreads(x);
  }
  static Attrs WriterBufferSize(int64 x) {
    return Attrs().WriterBufferSize(x);
  }
  static Attrs ShuffleOnRead(bool x) {
    return Attrs().ShuffleOnRead(x);
  }
  static Attrs Seed(int64 x) {
    return Attrs().Seed(x);
  }
  static Attrs Seed2(int64 x) {
    return Attrs().Seed2(x);
  }
  static Attrs Mode(StringPiece x) {
    return Attrs().Mode(x);
  }
  static Attrs SnapshotName(StringPiece x) {
    return Attrs().SnapshotName(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class SnapshotDatasetReader {
 public:
  /// Optional attribute setters for SnapshotDatasetReader
  struct Attrs {
    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs Compression(StringPiece x) {
      Attrs ret = *this;
      ret.compression_ = x;
      return ret;
    }

    StringPiece compression_ = "";
  };
  SnapshotDatasetReader(const ::tensorflow::Scope& scope, ::tensorflow::Input
                      shard_dir, ::tensorflow::Input start_index, const
                      DataTypeSlice& output_types, const
                      gtl::ArraySlice<PartialTensorShape>& output_shapes, int64
                      version);
  SnapshotDatasetReader(const ::tensorflow::Scope& scope, ::tensorflow::Input
                      shard_dir, ::tensorflow::Input start_index, const
                      DataTypeSlice& output_types, const
                      gtl::ArraySlice<PartialTensorShape>& output_shapes, int64
                      version, const SnapshotDatasetReader::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs Compression(StringPiece x) {
    return Attrs().Compression(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that will write to / read from a snapshot.
///
/// This dataset attempts to determine whether a valid snapshot exists at the
/// `snapshot_path`, and reads from the snapshot in lieu of using `input_dataset`.
/// If not, it will run the preprocessing pipeline as usual, and write out a
/// snapshot of the data processed for future use.
///
/// Args:
/// * scope: A Scope object
/// * input_dataset: A variant tensor representing the input dataset.
/// * path: The path we should write snapshots to / read snapshots from.
/// * reader_func: Optional. A function to control how to read data from snapshot shards.
/// * shard_func: Optional. A function to control how to shard data when writing a snapshot.
///
/// Optional attributes (see `Attrs`):
/// * compression: The type of compression to be applied to the saved snapshot files.
///
/// Returns:
/// * `Output`: The handle tensor.
class SnapshotDatasetV2 {
 public:
  /// Optional attribute setters for SnapshotDatasetV2
  struct Attrs {
    /// The type of compression to be applied to the saved snapshot files.
    ///
    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs Compression(StringPiece x) {
      Attrs ret = *this;
      ret.compression_ = x;
      return ret;
    }

    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs ReaderPrefix(StringPiece x) {
      Attrs ret = *this;
      ret.reader_prefix_ = x;
      return ret;
    }

    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs WriterPrefix(StringPiece x) {
      Attrs ret = *this;
      ret.writer_prefix_ = x;
      return ret;
    }

    /// Defaults to false
    TF_MUST_USE_RESULT Attrs HashValid(bool x) {
      Attrs ret = *this;
      ret.hash_valid_ = x;
      return ret;
    }

    /// Defaults to 0
    TF_MUST_USE_RESULT Attrs Hash(int64 x) {
      Attrs ret = *this;
      ret.hash_ = x;
      return ret;
    }

    StringPiece compression_ = "";
    StringPiece reader_prefix_ = "";
    StringPiece writer_prefix_ = "";
    bool hash_valid_ = false;
    int64 hash_ = 0;
  };
  SnapshotDatasetV2(const ::tensorflow::Scope& scope, ::tensorflow::Input
                  input_dataset, ::tensorflow::Input path,
                  ::tensorflow::InputList reader_func_other_args,
                  ::tensorflow::InputList shard_func_other_args, const
                  DataTypeSlice& output_types, const
                  gtl::ArraySlice<PartialTensorShape>& output_shapes, const
                  NameAttrList& reader_func, const NameAttrList& shard_func);
  SnapshotDatasetV2(const ::tensorflow::Scope& scope, ::tensorflow::Input
                  input_dataset, ::tensorflow::Input path,
                  ::tensorflow::InputList reader_func_other_args,
                  ::tensorflow::InputList shard_func_other_args, const
                  DataTypeSlice& output_types, const
                  gtl::ArraySlice<PartialTensorShape>& output_shapes, const
                  NameAttrList& reader_func, const NameAttrList& shard_func,
                  const SnapshotDatasetV2::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs Compression(StringPiece x) {
    return Attrs().Compression(x);
  }
  static Attrs ReaderPrefix(StringPiece x) {
    return Attrs().ReaderPrefix(x);
  }
  static Attrs WriterPrefix(StringPiece x) {
    return Attrs().WriterPrefix(x);
  }
  static Attrs HashValid(bool x) {
    return Attrs().HashValid(x);
  }
  static Attrs Hash(int64 x) {
    return Attrs().Hash(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class SnapshotNestedDatasetReader {
 public:
  SnapshotNestedDatasetReader(const ::tensorflow::Scope& scope,
                            ::tensorflow::InputList inputs, const
                            DataTypeSlice& output_types, const
                            gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that executes a SQL query and emits rows of the result set.
///
/// Args:
/// * scope: A Scope object
/// * driver_name: The database type. Currently, the only supported type is 'sqlite'.
/// * data_source_name: A connection string to connect to the database.
/// * query: A SQL query to execute.
///
/// Returns:
/// * `Output`: The handle tensor.
class SqlDataset {
 public:
  SqlDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input driver_name,
           ::tensorflow::Input data_source_name, ::tensorflow::Input query,
           const DataTypeSlice& output_types, const
           gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a statistics manager resource.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class StatsAggregatorHandle {
 public:
  /// Optional attribute setters for StatsAggregatorHandle
  struct Attrs {
    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs Container(StringPiece x) {
      Attrs ret = *this;
      ret.container_ = x;
      return ret;
    }

    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs SharedName(StringPiece x) {
      Attrs ret = *this;
      ret.shared_name_ = x;
      return ret;
    }

    StringPiece container_ = "";
    StringPiece shared_name_ = "";
  };
  StatsAggregatorHandle(const ::tensorflow::Scope& scope);
  StatsAggregatorHandle(const ::tensorflow::Scope& scope, const
                      StatsAggregatorHandle::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs Container(StringPiece x) {
    return Attrs().Container(x);
  }
  static Attrs SharedName(StringPiece x) {
    return Attrs().SharedName(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// TODO: add doc.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class StatsAggregatorHandleV2 {
 public:
  /// Optional attribute setters for StatsAggregatorHandleV2
  struct Attrs {
    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs Container(StringPiece x) {
      Attrs ret = *this;
      ret.container_ = x;
      return ret;
    }

    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs SharedName(StringPiece x) {
      Attrs ret = *this;
      ret.shared_name_ = x;
      return ret;
    }

    StringPiece container_ = "";
    StringPiece shared_name_ = "";
  };
  StatsAggregatorHandleV2(const ::tensorflow::Scope& scope);
  StatsAggregatorHandleV2(const ::tensorflow::Scope& scope, const
                        StatsAggregatorHandleV2::Attrs& attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs Container(StringPiece x) {
    return Attrs().Container(x);
  }
  static Attrs SharedName(StringPiece x) {
    return Attrs().SharedName(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Set a summary_writer_interface to record statistics using given stats_aggregator.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * the created `Operation`
class StatsAggregatorSetSummaryWriter {
 public:
  StatsAggregatorSetSummaryWriter(const ::tensorflow::Scope& scope,
                                ::tensorflow::Input stats_aggregator,
                                ::tensorflow::Input summary);
  operator ::tensorflow::Operation() const { return operation; }

  Operation operation;
};

/// Produces a summary of any statistics recorded by the given statistics manager.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The summary tensor.
class StatsAggregatorSummary {
 public:
  StatsAggregatorSummary(const ::tensorflow::Scope& scope, ::tensorflow::Input
                       iterator);
  operator ::tensorflow::Output() const { return summary; }
  operator ::tensorflow::Input() const { return summary; }
  ::tensorflow::Node* node() const { return summary.node(); }

  Operation operation;
  ::tensorflow::Output summary;
};

/// Creates a dataset that stops iteration when predicate` is false.
///
/// The `predicate` function must return a scalar boolean and accept the
/// following arguments:
///
/// * One tensor for each component of an element of `input_dataset`.
/// * One tensor for each value in `other_arguments`.
///
/// Args:
/// * scope: A Scope object
/// * other_arguments: A list of tensors, typically values that were captured when
/// building a closure for `predicate`.
/// * predicate: A function returning a scalar boolean.
///
/// Returns:
/// * `Output`: The handle tensor.
class TakeWhileDataset {
 public:
  TakeWhileDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                 input_dataset, ::tensorflow::InputList other_arguments, const
                 NameAttrList& predicate, const DataTypeSlice& output_types,
                 const gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that uses a custom thread pool to compute `input_dataset`.
///
/// Args:
/// * scope: A Scope object
/// * thread_pool: A resource produced by the ThreadPoolHandle op.
///
/// Returns:
/// * `Output`: The handle tensor.
class ThreadPoolDataset {
 public:
  ThreadPoolDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
                  input_dataset, ::tensorflow::Input thread_pool, const
                  DataTypeSlice& output_types, const
                  gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Creates a dataset that uses a custom thread pool to compute `input_dataset`.
///
/// Args:
/// * scope: A Scope object
/// * num_threads: The number of threads in the thread pool.
/// * display_name: A human-readable name for the threads that may be visible in some
/// visualizations.
/// threadpool.
///
/// Optional attributes (see `Attrs`):
/// * max_intra_op_parallelism: The maximum degree of parallelism to use within operations that execute on this
/// threadpool.
///
/// Returns:
/// * `Output`: A resource that can be consumed by one or more ExperimentalThreadPoolDataset
/// ops.
class ThreadPoolHandle {
 public:
  /// Optional attribute setters for ThreadPoolHandle
  struct Attrs {
    /// The maximum degree of parallelism to use within operations that execute on this
    /// threadpool.
    ///
    /// Defaults to 1
    TF_MUST_USE_RESULT Attrs MaxIntraOpParallelism(int64 x) {
      Attrs ret = *this;
      ret.max_intra_op_parallelism_ = x;
      return ret;
    }

    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs Container(StringPiece x) {
      Attrs ret = *this;
      ret.container_ = x;
      return ret;
    }

    /// Defaults to ""
    TF_MUST_USE_RESULT Attrs SharedName(StringPiece x) {
      Attrs ret = *this;
      ret.shared_name_ = x;
      return ret;
    }

    int64 max_intra_op_parallelism_ = 1;
    StringPiece container_ = "";
    StringPiece shared_name_ = "";
  };
  ThreadPoolHandle(const ::tensorflow::Scope& scope, int64 num_threads,
                 StringPiece display_name);
  ThreadPoolHandle(const ::tensorflow::Scope& scope, int64 num_threads,
                 StringPiece display_name, const ThreadPoolHandle::Attrs&
                 attrs);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  static Attrs MaxIntraOpParallelism(int64 x) {
    return Attrs().MaxIntraOpParallelism(x);
  }
  static Attrs Container(StringPiece x) {
    return Attrs().Container(x);
  }
  static Attrs SharedName(StringPiece x) {
    return Attrs().SharedName(x);
  }

  Operation operation;
  ::tensorflow::Output handle;
};

/// A dataset that splits the elements of its input into multiple elements.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class UnbatchDataset {
 public:
  UnbatchDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
               input_dataset, const DataTypeSlice& output_types, const
               gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

/// Uncompresses a compressed dataset element.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `OutputList`: The components tensor.
class UncompressElement {
 public:
  UncompressElement(const ::tensorflow::Scope& scope, ::tensorflow::Input
                  compressed, const DataTypeSlice& output_types, const
                  gtl::ArraySlice<PartialTensorShape>& output_shapes);
  ::tensorflow::Output operator[](size_t index) const { return components[index]; }


  Operation operation;
  ::tensorflow::OutputList components;
};

/// Creates a dataset that contains the unique elements of `input_dataset`.
///
/// Args:
/// * scope: A Scope object
///
/// Returns:
/// * `Output`: The handle tensor.
class UniqueDataset {
 public:
  UniqueDataset(const ::tensorflow::Scope& scope, ::tensorflow::Input
              input_dataset, const DataTypeSlice& output_types, const
              gtl::ArraySlice<PartialTensorShape>& output_shapes);
  operator ::tensorflow::Output() const { return handle; }
  operator ::tensorflow::Input() const { return handle; }
  ::tensorflow::Node* node() const { return handle.node(); }

  Operation operation;
  ::tensorflow::Output handle;
};

}  // namespace internal
}  // namespace ops
}  // namespace tensorflow

#endif  // TENSORFLOW_CC_OPS_EXPERIMENTAL_DATASET_OPS_INTERNAL_H_
